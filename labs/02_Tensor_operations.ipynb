{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhTm-wTJNJEJ"
      },
      "source": [
        "# Deep Learning & Applied AI\n",
        "\n",
        "# Tutorial 2: Tensor operations\n",
        "\n",
        "In this tutorial, we will cover:\n",
        "\n",
        "- Tensor operations: broadcasting, (not)-elementwise operations, tensor contraction, einsum\n",
        "\n",
        "Our info:\n",
        "\n",
        "- Based on original material by Dr. Antonio Norelli (norelli@di.uniroma1.it)\n",
        "\n",
        "Course:\n",
        "\n",
        "- Website and notebooks will be available at https://erodola.github.io/DLAI-s2-2025/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t5Enszg17Yr"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this tutorial we will continue to learn basic tensor usage, we will cover  broadcasting, fundamental linear algebra operations, and finally `einsum`, a single operation implementing the Einstein notation to rule them all!\n",
        "\n",
        "All these tensor operations will come in handy to build our deep neural networks.\n",
        "Yet, the high level API offered by PyTorch to perform GPU-acccelerated linear algebra operations may turn useful in many other fields, from microbiology to fluid dynamics.\n",
        "\n",
        "The GPU computing paradigm offers several benefits over single-core machines or traditional supercomputers equipped with many single-core nodes.\n",
        "Deep learning frameworks such as the one we are studying are a very good compromise between simplicity and expressivenes to unleash the power of GPU-computing.\n",
        "\n",
        "To get even more control you can tackle directly the CUDA language, but we won't go there with this course!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHLO4Z-T_yxB"
      },
      "source": [
        "## PyTorch\n",
        "\n",
        "**Reminder:** Familiarize with the [PyTorch Documentation](https://pytorch.org/docs/stable/) as it will greatly assist you.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pRePt-K1_yw9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "p0B2Y47YJY97"
      },
      "outputs": [],
      "source": [
        "# Utility print function\n",
        "\n",
        "from typing import Union\n",
        "\n",
        "def print_arr(*arr: Union[torch.Tensor, np.ndarray], prefix: str = \"\") -> None:\n",
        "    \"\"\"Pretty print tensors, together with their shape and type\n",
        "\n",
        "    :param arr: one or more tensors\n",
        "    :param prefix: prefix to use when printing the tensors\n",
        "    \"\"\"\n",
        "    print(\n",
        "        \"\\n\\n\".join(\n",
        "            f\"{prefix}{str(x)} <shape: {x.shape}> <dtype: {x.dtype}>\" for x in arr\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bie5eT1Md_FW"
      },
      "source": [
        "####Set torch and numpy random seeds for reproducibility\n",
        "\n",
        "As we will see, several operations in deep learning (e.g. training a network) rely on randomness in order to work effectively. This means that we will get different results each time we run a test, which can make design and debugging difficult.\n",
        "\n",
        "To this end, we usually **set a fixed seed** for the pseudo-random number generator, so that we are sure to always see the \"same randomness\" that makes our tests reproducible.\n",
        "\n",
        "> Once your model works, remember to test multiple times _without_ a fixed seed! The results you got at design time may be due to overfitting the seed (e.g. you have chosen hyperparameters that happen to work particularly well with a given seed.), or just out of luck.\n",
        "\n",
        "If you are going to use a gpu, two further options must be set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2tGN_bJOcfd3"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "np.random.seed(42)\n",
        "random.seed(0)\n",
        "\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "torch.backends.cudnn.deterministic = True  # Note that this Deterministic mode can have a performance impact\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# We will see frameworks that aid the reproducibility of your code,\n",
        "# e.g. PyTorch Lightning exposes a `seed_everything` function by default:\n",
        "# https://github.com/PyTorchLightning/pytorch-lightning/blob/e1f5eacab98670bc1de72c88657404a15aadd527/pytorch_lightning/utilities/seed.py#L29"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUl8vYRv_yuG"
      },
      "source": [
        "### **Tensor operations**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bFFU6Xw7_yuA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9d4a430-3045-4ce9-8353-de8c6811705f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4963, 0.7682, 0.0885],\n",
              "        [0.1320, 0.3074, 0.6341],\n",
              "        [0.4901, 0.8964, 0.4556]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "t = torch.rand(3,3)\n",
        "t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pMboSXXogDd"
      },
      "source": [
        "Functions that operate on tensors are often accessible in different ways:\n",
        "\n",
        "- From the **`torch` module**...:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Cpff92U3obK5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee02261b-bce9-4195-e565-7c1944fa9f05"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9925, 1.5364, 0.1770],\n",
              "        [0.2641, 0.6148, 1.2682],\n",
              "        [0.9802, 1.7929, 0.9113]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "torch.add(t, t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD99wCT8_yt0"
      },
      "source": [
        "- ...or by tensors **methods**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_zkKPFHo_ytw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d43cc46d-6d79-4a52-944d-be6948f3b725"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9925, 1.5364, 0.1770],\n",
              "        [0.2641, 0.6148, 1.2682],\n",
              "        [0.9802, 1.7929, 0.9113]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "t.add(t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TKgi2HL_yt_"
      },
      "source": [
        "- ...or even through **overloaded** operators:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HydKd9OK_yt7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c72a4d80-edee-41c3-dced-f3845b5666b5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9925, 1.5364, 0.1770],\n",
              "        [0.2641, 0.6148, 1.2682],\n",
              "        [0.9802, 1.7929, 0.9113]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "t + t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JyOSdbKfnkl"
      },
      "source": [
        "None of the above operates in-place:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FcRwwHVfoVOD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dc709a8-f188-487b-b364-fdebaaed0edb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4963, 0.7682, 0.0885],\n",
              "        [0.1320, 0.3074, 0.6341],\n",
              "        [0.4901, 0.8964, 0.4556]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# t is unchanged\n",
        "t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuJlpLP-gWMj"
      },
      "source": [
        "These functions are all equivalent, they are *aliases* of the same method.\n",
        "Personal preference, code consistency, and readability should guide your decision of which one to use.\n",
        "\n",
        "> e.g. `torch.add(...)` may be too verbose, but in some cases it may be preferable since it makes explicit to the code-reader that you are dealing with tensors. Nevertheless, if you are using [types](https://docs.python.org/3/library/typing.html) -- and you should be using types -- it will be rarely necessary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Amiof4a_yt6"
      },
      "source": [
        "\n",
        "Most operation in PyTorch are **not in-place**. It means that the resulting tensor is a *new* tensor, and it does not share the underlying data with other tensors. Changes to the new tensor are not reflected to other tensors.\n",
        "We will see in future tutorials why this is important, for now a TLDR is: **in-place operations may break the auto-differentiation mechanism**.\n",
        "\n",
        "\n",
        "In-place operations are still available in PyTorch, and in some cases (e.g. when you don't need autodiff) they can be useful; they are more efficient, since they never require to perform deep copies of the data.\n",
        "They are normally recognized by a trailing `_`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8NB70TkLiApB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "610888eb-638e-4776-f2d0-ccb614f06557"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4963, 0.7682, 0.0885],\n",
              "        [0.1320, 0.3074, 0.6341],\n",
              "        [0.4901, 0.8964, 0.4556]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7FKDL8ZG_yt2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24016077-8b26-4426-d42f-1cbbd6e36b19"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9925, 1.5364, 0.1770],\n",
              "        [0.2641, 0.6148, 1.2682],\n",
              "        [0.9802, 1.7929, 0.9113]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "t.add_(t)  # notice the trailing _"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YxG3NsBOiFN3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "440d5ee3-cdc3-4ba9-adc7-e2d4f30e6805"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9925, 1.5364, 0.1770],\n",
              "        [0.2641, 0.6148, 1.2682],\n",
              "        [0.9802, 1.7929, 0.9113]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "t  # t itself changed!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2dWLk0ji-vD"
      },
      "source": [
        "Another common in-place operation is the assignment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "IqYinZnGjB-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7124dc16-d557-40d2-c517-988bf4924a15"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[42.0000, 42.0000, 42.0000],\n",
              "        [ 0.2641,  0.6148,  1.2682],\n",
              "        [ 0.9802,  1.7929,  0.9113]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "t[0] = 42\n",
        "t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ9ChdjH_ytv"
      },
      "source": [
        "#### **Basic operations and broadcasting**\n",
        "\n",
        "Basic mathematical operations $(+, -, *, /, **)$ are applied **element-wise**: for example, if `x` and `y` are two tensors, the product `x*y` is a tensor with the same size, and its values are the element-wise products of the two tensors. In mathematics, this is also called a Hadamard product.\n",
        "\n",
        "**Broadcasting** is another powerful mechanism that allows PyTorch to perform operations on tensors of different shapes. The most basic example is summing a scalar (a rank-0 tensor) to a matrix (a rank-2 tensor)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "doNfhKA5_ytq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da1f2977-bdee-4909-a618-ee9d084ce914"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 6.,  8.],\n",
            "        [10., 12.]], dtype=torch.float64)\n",
            "tensor([[5.2000, 6.2000],\n",
            "        [7.2000, 8.2000]], dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor([[1, 2], [3, 4]], dtype=torch.float64)\n",
        "y = torch.tensor([[5, 6], [7, 8]], dtype=torch.float64)\n",
        "\n",
        "print(x + y)  # element-wise sum\n",
        "print(x + 4.2)  # broadcasting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "GlZG4i_D_ytj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2958556b-dcbb-47db-d1ec-b928ce86cf93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.,  7.],\n",
            "        [16., 27.]], dtype=torch.float64)\n",
            "tensor([[-0.8000, -0.6667],\n",
            "        [-0.5714, -0.5000]], dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "# other examples\n",
        "print(x * y - 5) # element wise\n",
        "print((x - y) / y)  # element-wise division!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rNTtIs2NMKt"
      },
      "source": [
        "Broadcasting is quite powerful! When you perform an operation between two tensors with different shape, PyTorch automatically \"broadcasts\" the smaller tensor across the larger tensor so that they have compatible shapes.\n",
        "\n",
        "In the example below, the sequence `v` is replicated (_without actually copying data!_) along the missing dimension so that it fits the shape of matrix `m`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4XzPdOKXNLV-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79cea830-31c4-4ae7-b91d-cb25bba3982f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0,  1,  2],\n",
            "        [ 3,  4,  5],\n",
            "        [ 6,  7,  8],\n",
            "        [ 9, 10, 11]]) <shape: torch.Size([4, 3])> <dtype: torch.int64>\n",
            "\n",
            "tensor([100,   0, 200]) <shape: torch.Size([3])> <dtype: torch.int64>\n",
            "\n",
            "tensor([[100,   1, 202],\n",
            "        [103,   4, 205],\n",
            "        [106,   7, 208],\n",
            "        [109,  10, 211]]) <shape: torch.Size([4, 3])> <dtype: torch.int64>\n"
          ]
        }
      ],
      "source": [
        "m = torch.arange(12).reshape(4, 3)\n",
        "v = torch.tensor([100, 0, 200])\n",
        "n = m + v\n",
        "print_arr(m, v, n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4IlFl_ZjybT"
      },
      "source": [
        "In this other example `m` and `u` are both rank-2, but the smaller one (`u`) is expanded along the dimension where it has size 1 to fit `m`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "CYREKQAuQNYy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6680678-4928-4dbc-de34-5785d8858d07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0,  1,  2],\n",
            "        [ 3,  4,  5],\n",
            "        [ 6,  7,  8],\n",
            "        [ 9, 10, 11]]) <shape: torch.Size([4, 3])> <dtype: torch.int64>\n",
            "\n",
            "tensor([[ 0],\n",
            "        [10],\n",
            "        [ 0],\n",
            "        [20]]) <shape: torch.Size([4, 1])> <dtype: torch.int64>\n",
            "\n",
            "tensor([[ 0,  1,  2],\n",
            "        [13, 14, 15],\n",
            "        [ 6,  7,  8],\n",
            "        [29, 30, 31]]) <shape: torch.Size([4, 3])> <dtype: torch.int64>\n"
          ]
        }
      ],
      "source": [
        "m = torch.arange(12).reshape(4, 3)\n",
        "u = torch.tensor([0, 10, 0, 20]).reshape(4,1)\n",
        "n = m + u\n",
        "print_arr(m, u, n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZoseFdYkTpG"
      },
      "source": [
        "In the following example, both tensors are expanded along their size-1 dimensions, so that the sum makes sense:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "ReNq-RtKg1Dy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5df51621-ce69-4d43-a03b-0401251cbf39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0],\n",
            "        [10],\n",
            "        [ 0],\n",
            "        [20]]) <shape: torch.Size([4, 1])> <dtype: torch.int64>\n",
            "\n",
            "tensor([100,   0, 200]) <shape: torch.Size([3])> <dtype: torch.int64>\n",
            "\n",
            "tensor([[100,   0, 200],\n",
            "        [110,  10, 210],\n",
            "        [100,   0, 200],\n",
            "        [120,  20, 220]]) <shape: torch.Size([4, 3])> <dtype: torch.int64>\n"
          ]
        }
      ],
      "source": [
        "w = u + v\n",
        "print_arr(u, v, w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYNysSHoQ6dX"
      },
      "source": [
        "Mastering broadcasting is hard!\n",
        "\n",
        "However, it is very convenient as it allows writing **vectorized** code, i.e., code that avoids explicit python loops which can not be efficiently parallelized.\n",
        "\n",
        "Technically, broadcasting takes advantage of the underlying C implementation of PyTorch and Numpy (on CPU) or CUDA implementation of Pytorch (on GPU). Here's a take-home illustration for your convenience:\n",
        "\n",
        "![broadcasting](https://jakevdp.github.io/PythonDataScienceHandbook/figures/02.05-broadcasting.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqyMVYPL_yoC"
      },
      "source": [
        "##### **EXERCISE**\n",
        ">\n",
        "> Given two vectors $x \\in \\mathbb{R}^n$ and $y \\in \\mathbb{R}^m$, compute the differences between all possible pairs of their elements, and organize these differences in a matrix $Z \\in \\mathbb{R}^{n \\times m}$:\n",
        "> $$ z_{ij} = x_i - y_j $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "-WqFqMkksOBg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb626ecb-bbb5-4443-c673-f1ea4f6d63fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-3, -4],\n",
            "        [-2, -3],\n",
            "        [-1, -2]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "x = torch.tensor([1, 2, 3])\n",
        "y = torch.tensor([4, 5])\n",
        "\n",
        "# ✏️ your code here\n",
        "t = torch.unsqueeze(x,1)\n",
        "z=t-y\n",
        "print(z)\n",
        "z.shape\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "gtF2--mB_yn1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af6fe6df-3101-42d2-80c6-5439190b0f02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3]) <shape: torch.Size([3])> <dtype: torch.int64>\n",
            "\n",
            "tensor([4, 5]) <shape: torch.Size([2])> <dtype: torch.int64>\n",
            "\n",
            "tensor([[-3, -4],\n",
            "        [-2, -3],\n",
            "        [-1, -2]]) <shape: torch.Size([3, 2])> <dtype: torch.int64>\n"
          ]
        }
      ],
      "source": [
        "# @title 👀 Solution\n",
        "\n",
        "\n",
        "out = x.unsqueeze(1) - y\n",
        "\n",
        "# equivalent to:\n",
        "# x.reshape([-1, 1]) - y\n",
        "\n",
        "# equivalent to:\n",
        "# x[:, None] - y\n",
        "\n",
        "# equivalent to:\n",
        "# x[:, None] - y[None, :]\n",
        "\n",
        "print_arr(x, y, out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezcEIBPFSlFa"
      },
      "source": [
        "#### 📖 **Broadcasting, let's take a peek under the hood**\n",
        "\n",
        "To recap: if a PyTorch operation supports broadcast, then **its tensor arguments can be implicitly expanded to be of equal sizes** (without making copies of the data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfTF3SiYSlFg"
      },
      "source": [
        "###### **Broadcastable tensors**\n",
        "\n",
        "Two tensors are \"broadcastable\" if:\n",
        "- Each tensor has at least one dimension\n",
        "- When iterating over the dimension sizes, starting at the trailing dimension, the dimension **sizes** must either **be equal**, **one of them is 1**, or **one of them does not exist**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mdgd4qM5SlFh"
      },
      "source": [
        "###### **Broadcasting rules**\n",
        "\n",
        "Broadcasting two tensors together follows these rules:\n",
        "\n",
        "1. If the input tensors have different ranks, **singleton dimensions are prepended to the shape** of the smaller one until it has the same rank as the other\n",
        "2. The size in each dimension of the **output shape** is the maximum size in that dimension between the two tensors\n",
        "3. An input can be used in the computation if its size in a particular **dimension either matches** the output size in that dimension, **or is a singleton dimension**\n",
        "4. If an input has a dimension size of 1 in its shape, the **first data entry in that dimension will be used for all calculations** along that dimension."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcj42Be5SlFi"
      },
      "source": [
        "**Example**:\n",
        "\n",
        "- `m` has shape `[4, 3]`\n",
        "- `v` has shape `[3,]`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "lXQWIJtoSlFj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b99c5cd-ace9-4bf6-d3b4-2bf9bd3eae92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0,  1,  2],\n",
            "        [ 3,  4,  5],\n",
            "        [ 6,  7,  8],\n",
            "        [ 9, 10, 11]]) <shape: torch.Size([4, 3])> <dtype: torch.int64>\n",
            "\n",
            "tensor([100,   0, 200]) <shape: torch.Size([3])> <dtype: torch.int64>\n"
          ]
        }
      ],
      "source": [
        "print_arr(m, v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "MEdjjDButCVa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84303d7e-d021-4396-f7a4-58cb0f6937dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[100,   1, 202],\n",
            "        [103,   4, 205],\n",
            "        [106,   7, 208],\n",
            "        [109,  10, 211]]) <shape: torch.Size([4, 3])> <dtype: torch.int64>\n"
          ]
        }
      ],
      "source": [
        "n = m + v\n",
        "print_arr(n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZLeIamSSlFn"
      },
      "source": [
        "\n",
        "Following the Broadcasting logic, this is what happened:\n",
        "\n",
        "- `v` has less dims than `m` so a dimension of `1` is **prepended** $\\to$ `v` is now `[1, 3]`.\n",
        "- Output shape will be `[max(1, 4), max(3, 3)] = [4, 3]`.\n",
        "- `dim 1` of `v` matches exactly `3`; `dim 0` is `1`, so we can use the first data entry in that dimension (i.e. the whole `row 0` of `v`) each time any row is accessed. This is effectively like converting `v` from `[1, 3]` to `[4, 3]` by stacking the repeated row four times."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeoux-PvSlFp"
      },
      "source": [
        "\n",
        "For more on broadcasting, see the [documentation](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html).\n",
        "\n",
        "Functions that support broadcasting are known as universal functions (i.e. ufuncs). For Numpy you can find the list of all universal functions in the [documentation](https://docs.scipy.org/doc/numpy/reference/ufuncs.html#available-ufuncs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_L_GYRgsnLo"
      },
      "source": [
        "#### **EXERCISE**\n",
        ">\n",
        "> Given a tensor $Y \\in \\mathbb{R}^{n \\times m}$ and an index pair $(a,b)$, for each element of $Y$ compute its $L_p$ distance to $(a,b)$, and store the resulting distance value in the corresponding cell of $Y$.\n",
        ">\n",
        "> In brief, compute:\n",
        "> $$ y_{ij} = d_{L_p}\\left( (i,j), (a,b) \\right) \\text{ for all }  i,j$$\n",
        ">\n",
        "> and visualize the resulting $Y$.\n",
        ">\n",
        "> Try different values of $p>0$ to see what happens.\n",
        ">\n",
        "> ---\n",
        ">\n",
        "> The [$L_p$ distance](https://en.wikipedia.org/wiki/Lp_space#The_p-norm_in_finite_dimensions) between two points $x$ and $y$ can be computed as: $d_{L_p}(x, y)=\\left( \\sum_{i=1}^n|x_i - y_i|^p\\right)^{1/p}$\n",
        ">\n",
        "> Example: The $L_1$ distance between $(i,j) = (3, 5)$ and $(a,b) = (14, 20)$ is:\n",
        "> $$ y_{3,5} = d_{L_1}( (3, 5), (14, 20) ) = |3 - 14| + |5 - 20| $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mdW4Xf964XQ1"
      },
      "outputs": [],
      "source": [
        "# @title Utility function, you can execute and safely ignore this cell\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "def plot_row_images(images: Union[torch.Tensor, np.ndarray]) -> None:\n",
        "  \"\"\" Plots the images in a subplot with multiple rows.\n",
        "\n",
        "  Handles correctly grayscale images.\n",
        "\n",
        "  :param images: tensor with shape [number of images, width, height, <colors>]\n",
        "  \"\"\"\n",
        "  from plotly.subplots import make_subplots\n",
        "  import plotly.graph_objects as go\n",
        "  fig = make_subplots(rows=1, cols=images.shape[0] ,\n",
        "                      specs=[[{}] * images.shape[0]])\n",
        "\n",
        "  # Convert grayscale image to something that go.Image likes\n",
        "  if images.dim() == 3:\n",
        "    images = torch.stack((images, images, images), dim= -1)\n",
        "  elif (images.dim() == 4 and images.shape[-1] == 1):\n",
        "    images = torch.cat((images, images, images), dim= -1)\n",
        "\n",
        "  assert images.shape[-1] == 3 or images.shape[-1] == 4\n",
        "\n",
        "  for i in range(images.shape[0]):\n",
        "    i_image = np.asarray(images[i, ...])\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Image(z = i_image, zmin=[0, 0, 0, 0], zmax=[1, 1, 1, 1]),\n",
        "        row=1, col=i + 1\n",
        "    )\n",
        "\n",
        "  fig.show()\n",
        "\n",
        "\n",
        "# When using plotly pay attention that it often does not like PyTorch Tensors\n",
        "# ...and it does not give any error, just a empty plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ApPt8XdAuK7R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "f3fea24b-cfd1-4045-dfde-6e48722c942e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"5673a45e-6979-4d25-a0d0-c845897f04cd\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"5673a45e-6979-4d25-a0d0-c845897f04cd\")) {                    Plotly.newPlot(                        \"5673a45e-6979-4d25-a0d0-c845897f04cd\",                        [{\"z\":[[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[1.0,1.0,1.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]]],\"zmax\":[1,1,1,1],\"zmin\":[0,0,0,0],\"type\":\"image\",\"xaxis\":\"x\",\"yaxis\":\"y\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0]}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('5673a45e-6979-4d25-a0d0-c845897f04cd');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "x = torch.zeros(50, 50)\n",
        "a = 25\n",
        "b = 25\n",
        "\n",
        "x[a, b] = 1  # this will be overwritten by your distance-calculating code\n",
        "plot_row_images(x[None, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "mFwiTbVV7iho",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "7eac30ee-8ad4-459e-8ef5-d794911101bc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"e185c29b-7edb-4536-996c-9abbdae3ee65\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"e185c29b-7edb-4536-996c-9abbdae3ee65\")) {                    Plotly.newPlot(                        \"e185c29b-7edb-4536-996c-9abbdae3ee65\",                        [{\"z\":[[[0.0,0.0,0.0],[0.018175125,0.018175125,0.018175125],[0.03269446,0.03269446,0.03269446],[0.043759525,0.043759525,0.043759525],[0.051806152,0.051806152,0.051806152],[0.057402194,0.057402194,0.057402194],[0.06113547,0.06113547,0.06113547],[0.06353134,0.06353134,0.06353134],[0.06501317,0.06501317,0.06501317],[0.06589687,0.06589687,0.06589687],[0.06640434,0.06640434,0.06640434],[0.066684365,0.066684365,0.066684365],[0.066832244,0.066832244,0.066832244],[0.06690651,0.06690651,0.06690651],[0.06694168,0.06694168,0.06694168],[0.066957235,0.066957235,0.066957235],[0.06696367,0.06696367,0.06696367],[0.066966,0.066966,0.066966],[0.06696677,0.06696677,0.06696677],[0.06696701,0.06696701,0.06696701],[0.06696701,0.06696701,0.06696701],[0.06696701,0.06696701,0.06696701],[0.06696701,0.06696701,0.06696701],[0.06696701,0.06696701,0.06696701],[0.06696701,0.06696701,0.06696701],[0.06696701,0.06696701,0.06696701],[0.06696701,0.06696701,0.06696701],[0.06696701,0.06696701,0.06696701],[0.06696701,0.06696701,0.06696701],[0.06696701,0.06696701,0.06696701],[0.06696701,0.06696701,0.06696701],[0.06696701,0.06696701,0.06696701],[0.06696677,0.06696677,0.06696677],[0.066966,0.066966,0.066966],[0.06696367,0.06696367,0.06696367],[0.066957235,0.066957235,0.066957235],[0.06694168,0.06694168,0.06694168],[0.06690651,0.06690651,0.06690651],[0.066832244,0.066832244,0.066832244],[0.066684365,0.066684365,0.066684365],[0.06640434,0.06640434,0.06640434],[0.06589687,0.06589687,0.06589687],[0.06501317,0.06501317,0.06501317],[0.06353134,0.06353134,0.06353134],[0.06113547,0.06113547,0.06113547],[0.057402194,0.057402194,0.057402194],[0.051806152,0.051806152,0.051806152],[0.043759525,0.043759525,0.043759525],[0.03269446,0.03269446,0.03269446],[0.018175125,0.018175125,0.018175125]],[[0.018175125,0.018175125,0.018175125],[0.04000002,0.04000002,0.04000002],[0.058098435,0.058098435,0.058098435],[0.07239407,0.07239407,0.07239407],[0.08312279,0.08312279,0.08312279],[0.0907771,0.0907771,0.0907771],[0.095982194,0.095982194,0.095982194],[0.09936774,0.09936774,0.09936774],[0.101480305,0.101480305,0.101480305],[0.10274702,0.10274702,0.10274702],[0.103477,0.103477,0.103477],[0.103880584,0.103880584,0.103880584],[0.10409379,0.10409379,0.10409379],[0.1042009,0.1042009,0.1042009],[0.10425174,0.10425174,0.10425174],[0.10427421,0.10427421,0.10427421],[0.10428339,0.10428339,0.10428339],[0.10428679,0.10428679,0.10428679],[0.10428798,0.10428798,0.10428798],[0.10428822,0.10428822,0.10428822],[0.10428834,0.10428834,0.10428834],[0.10428834,0.10428834,0.10428834],[0.10428834,0.10428834,0.10428834],[0.10428834,0.10428834,0.10428834],[0.10428834,0.10428834,0.10428834],[0.10428834,0.10428834,0.10428834],[0.10428834,0.10428834,0.10428834],[0.10428834,0.10428834,0.10428834],[0.10428834,0.10428834,0.10428834],[0.10428834,0.10428834,0.10428834],[0.10428834,0.10428834,0.10428834],[0.10428822,0.10428822,0.10428822],[0.10428798,0.10428798,0.10428798],[0.10428679,0.10428679,0.10428679],[0.10428339,0.10428339,0.10428339],[0.10427421,0.10427421,0.10427421],[0.10425174,0.10425174,0.10425174],[0.1042009,0.1042009,0.1042009],[0.10409379,0.10409379,0.10409379],[0.103880584,0.103880584,0.103880584],[0.103477,0.103477,0.103477],[0.10274702,0.10274702,0.10274702],[0.101480305,0.101480305,0.101480305],[0.09936774,0.09936774,0.09936774],[0.095982194,0.095982194,0.095982194],[0.0907771,0.0907771,0.0907771],[0.08312279,0.08312279,0.08312279],[0.07239407,0.07239407,0.07239407],[0.058098435,0.058098435,0.058098435],[0.04000002,0.04000002,0.04000002]],[[0.03269446,0.03269446,0.03269446],[0.058098435,0.058098435,0.058098435],[0.07999998,0.07999998,0.07999998],[0.09801525,0.09801525,0.09801525],[0.11206865,0.11206865,0.11206865],[0.122437775,0.122437775,0.122437775],[0.12967998,0.12967998,0.12967998],[0.13448352,0.13448352,0.13448352],[0.13752115,0.13752115,0.13752115],[0.1393581,0.1393581,0.1393581],[0.14042222,0.14042222,0.14042222],[0.14101219,0.14101219,0.14101219],[0.14132446,0.14132446,0.14132446],[0.1414814,0.1414814,0.1414814],[0.14155596,0.14155596,0.14155596],[0.14158893,0.14158893,0.14158893],[0.14160246,0.14160246,0.14160246],[0.1416074,0.1416074,0.1416074],[0.14160907,0.14160907,0.14160907],[0.14160955,0.14160955,0.14160955],[0.14160961,0.14160961,0.14160961],[0.14160961,0.14160961,0.14160961],[0.14160961,0.14160961,0.14160961],[0.14160961,0.14160961,0.14160961],[0.14160961,0.14160961,0.14160961],[0.14160961,0.14160961,0.14160961],[0.14160961,0.14160961,0.14160961],[0.14160961,0.14160961,0.14160961],[0.14160961,0.14160961,0.14160961],[0.14160961,0.14160961,0.14160961],[0.14160961,0.14160961,0.14160961],[0.14160955,0.14160955,0.14160955],[0.14160907,0.14160907,0.14160907],[0.1416074,0.1416074,0.1416074],[0.14160246,0.14160246,0.14160246],[0.14158893,0.14158893,0.14158893],[0.14155596,0.14155596,0.14155596],[0.1414814,0.1414814,0.1414814],[0.14132446,0.14132446,0.14132446],[0.14101219,0.14101219,0.14101219],[0.14042222,0.14042222,0.14042222],[0.1393581,0.1393581,0.1393581],[0.13752115,0.13752115,0.13752115],[0.13448352,0.13448352,0.13448352],[0.12967998,0.12967998,0.12967998],[0.122437775,0.122437775,0.122437775],[0.11206865,0.11206865,0.11206865],[0.09801525,0.09801525,0.09801525],[0.07999998,0.07999998,0.07999998],[0.058098435,0.058098435,0.058098435]],[[0.043759525,0.043759525,0.043759525],[0.07239407,0.07239407,0.07239407],[0.09801525,0.09801525,0.09801525],[0.120000005,0.120000005,0.120000005],[0.13792437,0.13792437,0.13792437],[0.15171522,0.15171522,0.15171522],[0.16169941,0.16169941,0.16169941],[0.16850913,0.16850913,0.16850913],[0.17290193,0.17290193,0.17290193],[0.17559367,0.17559367,0.17559367],[0.17716551,0.17716551,0.17716551],[0.17804116,0.17804116,0.17804116],[0.17850584,0.17850584,0.17850584],[0.17873973,0.17873973,0.17873973],[0.17885089,0.17885089,0.17885089],[0.17890006,0.17890006,0.17890006],[0.17892021,0.17892021,0.17892021],[0.17892766,0.17892766,0.17892766],[0.1789301,0.1789301,0.1789301],[0.17893082,0.17893082,0.17893082],[0.17893094,0.17893094,0.17893094],[0.17893094,0.17893094,0.17893094],[0.17893094,0.17893094,0.17893094],[0.17893094,0.17893094,0.17893094],[0.17893094,0.17893094,0.17893094],[0.17893094,0.17893094,0.17893094],[0.17893094,0.17893094,0.17893094],[0.17893094,0.17893094,0.17893094],[0.17893094,0.17893094,0.17893094],[0.17893094,0.17893094,0.17893094],[0.17893094,0.17893094,0.17893094],[0.17893082,0.17893082,0.17893082],[0.1789301,0.1789301,0.1789301],[0.17892766,0.17892766,0.17892766],[0.17892021,0.17892021,0.17892021],[0.17890006,0.17890006,0.17890006],[0.17885089,0.17885089,0.17885089],[0.17873973,0.17873973,0.17873973],[0.17850584,0.17850584,0.17850584],[0.17804116,0.17804116,0.17804116],[0.17716551,0.17716551,0.17716551],[0.17559367,0.17559367,0.17559367],[0.17290193,0.17290193,0.17290193],[0.16850913,0.16850913,0.16850913],[0.16169941,0.16169941,0.16169941],[0.15171522,0.15171522,0.15171522],[0.13792437,0.13792437,0.13792437],[0.120000005,0.120000005,0.120000005],[0.09801525,0.09801525,0.09801525],[0.07239407,0.07239407,0.07239407]],[[0.051806152,0.051806152,0.051806152],[0.08312279,0.08312279,0.08312279],[0.11206865,0.11206865,0.11206865],[0.13792437,0.13792437,0.13792437],[0.16000003,0.16000003,0.16000003],[0.17782497,0.17782497,0.17782497],[0.19133008,0.19133008,0.19133008],[0.20090163,0.20090163,0.20090163],[0.20725858,0.20725858,0.20725858],[0.21123308,0.21123308,0.21123308],[0.21358395,0.21358395,0.21358395],[0.21490365,0.21490365,0.21490365],[0.21560693,0.21560693,0.21560693],[0.21596181,0.21596181,0.21596181],[0.21613055,0.21613055,0.21613055],[0.21620536,0.21620536,0.21620536],[0.21623588,0.21623588,0.21623588],[0.21624726,0.21624726,0.21624726],[0.21625096,0.21625096,0.21625096],[0.21625197,0.21625197,0.21625197],[0.21625227,0.21625227,0.21625227],[0.21625227,0.21625227,0.21625227],[0.21625227,0.21625227,0.21625227],[0.21625227,0.21625227,0.21625227],[0.21625227,0.21625227,0.21625227],[0.21625227,0.21625227,0.21625227],[0.21625227,0.21625227,0.21625227],[0.21625227,0.21625227,0.21625227],[0.21625227,0.21625227,0.21625227],[0.21625227,0.21625227,0.21625227],[0.21625227,0.21625227,0.21625227],[0.21625197,0.21625197,0.21625197],[0.21625096,0.21625096,0.21625096],[0.21624726,0.21624726,0.21624726],[0.21623588,0.21623588,0.21623588],[0.21620536,0.21620536,0.21620536],[0.21613055,0.21613055,0.21613055],[0.21596181,0.21596181,0.21596181],[0.21560693,0.21560693,0.21560693],[0.21490365,0.21490365,0.21490365],[0.21358395,0.21358395,0.21358395],[0.21123308,0.21123308,0.21123308],[0.20725858,0.20725858,0.20725858],[0.20090163,0.20090163,0.20090163],[0.19133008,0.19133008,0.19133008],[0.17782497,0.17782497,0.17782497],[0.16000003,0.16000003,0.16000003],[0.13792437,0.13792437,0.13792437],[0.11206865,0.11206865,0.11206865],[0.08312279,0.08312279,0.08312279]],[[0.057402194,0.057402194,0.057402194],[0.0907771,0.0907771,0.0907771],[0.122437775,0.122437775,0.122437775],[0.15171522,0.15171522,0.15171522],[0.17782497,0.17782497,0.17782497],[0.19999999,0.19999999,0.19999999],[0.21771562,0.21771562,0.21771562],[0.23090887,0.23090887,0.23090887],[0.2400381,0.2400381,0.2400381],[0.24592173,0.24592173,0.24592173],[0.24947315,0.24947315,0.24947315],[0.2514915,0.2514915,0.2514915],[0.2525748,0.2525748,0.2525748],[0.25312346,0.25312346,0.25312346],[0.25338483,0.25338483,0.25338483],[0.25350076,0.25350076,0.25350076],[0.25354826,0.25354826,0.25354826],[0.25356585,0.25356585,0.25356585],[0.25357157,0.25357157,0.25357157],[0.25357318,0.25357318,0.25357318],[0.2535736,0.2535736,0.2535736],[0.2535736,0.2535736,0.2535736],[0.25357366,0.25357366,0.25357366],[0.25357366,0.25357366,0.25357366],[0.25357366,0.25357366,0.25357366],[0.25357366,0.25357366,0.25357366],[0.25357366,0.25357366,0.25357366],[0.25357366,0.25357366,0.25357366],[0.25357366,0.25357366,0.25357366],[0.2535736,0.2535736,0.2535736],[0.2535736,0.2535736,0.2535736],[0.25357318,0.25357318,0.25357318],[0.25357157,0.25357157,0.25357157],[0.25356585,0.25356585,0.25356585],[0.25354826,0.25354826,0.25354826],[0.25350076,0.25350076,0.25350076],[0.25338483,0.25338483,0.25338483],[0.25312346,0.25312346,0.25312346],[0.2525748,0.2525748,0.2525748],[0.2514915,0.2514915,0.2514915],[0.24947315,0.24947315,0.24947315],[0.24592173,0.24592173,0.24592173],[0.2400381,0.2400381,0.2400381],[0.23090887,0.23090887,0.23090887],[0.21771562,0.21771562,0.21771562],[0.19999999,0.19999999,0.19999999],[0.17782497,0.17782497,0.17782497],[0.15171522,0.15171522,0.15171522],[0.122437775,0.122437775,0.122437775],[0.0907771,0.0907771,0.0907771]],[[0.06113547,0.06113547,0.06113547],[0.095982194,0.095982194,0.095982194],[0.12967998,0.12967998,0.12967998],[0.16169941,0.16169941,0.16169941],[0.19133008,0.19133008,0.19133008],[0.21771562,0.21771562,0.21771562],[0.24000001,0.24000001,0.24000001],[0.25759476,0.25759476,0.25759476],[0.27044666,0.27044666,0.27044666],[0.2791012,0.2791012,0.2791012],[0.28449202,0.28449202,0.28449202],[0.28761846,0.28761846,0.28761846],[0.28931636,0.28931636,0.28931636],[0.29018205,0.29018205,0.29018205],[0.29059547,0.29059547,0.29059547],[0.29077935,0.29077935,0.29077935],[0.29085463,0.29085463,0.29085463],[0.29088253,0.29088253,0.29088253],[0.2908917,0.2908917,0.2908917],[0.29089427,0.29089427,0.29089427],[0.2908948,0.2908948,0.2908948],[0.29089493,0.29089493,0.29089493],[0.290895,0.290895,0.290895],[0.290895,0.290895,0.290895],[0.290895,0.290895,0.290895],[0.290895,0.290895,0.290895],[0.290895,0.290895,0.290895],[0.290895,0.290895,0.290895],[0.290895,0.290895,0.290895],[0.29089493,0.29089493,0.29089493],[0.2908948,0.2908948,0.2908948],[0.29089427,0.29089427,0.29089427],[0.2908917,0.2908917,0.2908917],[0.29088253,0.29088253,0.29088253],[0.29085463,0.29085463,0.29085463],[0.29077935,0.29077935,0.29077935],[0.29059547,0.29059547,0.29059547],[0.29018205,0.29018205,0.29018205],[0.28931636,0.28931636,0.28931636],[0.28761846,0.28761846,0.28761846],[0.28449202,0.28449202,0.28449202],[0.2791012,0.2791012,0.2791012],[0.27044666,0.27044666,0.27044666],[0.25759476,0.25759476,0.25759476],[0.24000001,0.24000001,0.24000001],[0.21771562,0.21771562,0.21771562],[0.19133008,0.19133008,0.19133008],[0.16169941,0.16169941,0.16169941],[0.12967998,0.12967998,0.12967998],[0.095982194,0.095982194,0.095982194]],[[0.06353134,0.06353134,0.06353134],[0.09936774,0.09936774,0.09936774],[0.13448352,0.13448352,0.13448352],[0.16850913,0.16850913,0.16850913],[0.20090163,0.20090163,0.20090163],[0.23090887,0.23090887,0.23090887],[0.25759476,0.25759476,0.25759476],[0.27999997,0.27999997,0.27999997],[0.29746062,0.29746062,0.29746062],[0.30993742,0.30993742,0.30993742],[0.31808287,0.31808287,0.31808287],[0.3229627,0.3229627,0.3229627],[0.32566643,0.32566643,0.32566643],[0.32706022,0.32706022,0.32706022],[0.32772982,0.32772982,0.32772982],[0.32802838,0.32802838,0.32802838],[0.32815063,0.32815063,0.32815063],[0.32819605,0.32819605,0.32819605],[0.32821095,0.32821095,0.32821095],[0.32821512,0.32821512,0.32821512],[0.32821608,0.32821608,0.32821608],[0.3282162,0.3282162,0.3282162],[0.3282163,0.3282163,0.3282163],[0.3282163,0.3282163,0.3282163],[0.3282163,0.3282163,0.3282163],[0.3282163,0.3282163,0.3282163],[0.3282163,0.3282163,0.3282163],[0.3282163,0.3282163,0.3282163],[0.3282163,0.3282163,0.3282163],[0.3282162,0.3282162,0.3282162],[0.32821608,0.32821608,0.32821608],[0.32821512,0.32821512,0.32821512],[0.32821095,0.32821095,0.32821095],[0.32819605,0.32819605,0.32819605],[0.32815063,0.32815063,0.32815063],[0.32802838,0.32802838,0.32802838],[0.32772982,0.32772982,0.32772982],[0.32706022,0.32706022,0.32706022],[0.32566643,0.32566643,0.32566643],[0.3229627,0.3229627,0.3229627],[0.31808287,0.31808287,0.31808287],[0.30993742,0.30993742,0.30993742],[0.29746062,0.29746062,0.29746062],[0.27999997,0.27999997,0.27999997],[0.25759476,0.25759476,0.25759476],[0.23090887,0.23090887,0.23090887],[0.20090163,0.20090163,0.20090163],[0.16850913,0.16850913,0.16850913],[0.13448352,0.13448352,0.13448352],[0.09936774,0.09936774,0.09936774]],[[0.06501317,0.06501317,0.06501317],[0.101480305,0.101480305,0.101480305],[0.13752115,0.13752115,0.13752115],[0.17290193,0.17290193,0.17290193],[0.20725858,0.20725858,0.20725858],[0.2400381,0.2400381,0.2400381],[0.27044666,0.27044666,0.27044666],[0.29746062,0.29746062,0.29746062],[0.32,0.32,0.32],[0.3373109,0.3373109,0.3373109],[0.34937418,0.34937418,0.34937418],[0.35697365,0.35697365,0.35697365],[0.36132693,0.36132693,0.36132693],[0.3636154,0.3636154,0.3636154],[0.36472595,0.36472595,0.36472595],[0.36522353,0.36522353,0.36522353],[0.3654279,0.3654279,0.3654279],[0.3655038,0.3655038,0.3655038],[0.3655287,0.3655287,0.3655287],[0.36553568,0.36553568,0.36553568],[0.36553723,0.36553723,0.36553723],[0.36553752,0.36553752,0.36553752],[0.36553758,0.36553758,0.36553758],[0.36553758,0.36553758,0.36553758],[0.36553758,0.36553758,0.36553758],[0.36553758,0.36553758,0.36553758],[0.36553758,0.36553758,0.36553758],[0.36553758,0.36553758,0.36553758],[0.36553758,0.36553758,0.36553758],[0.36553752,0.36553752,0.36553752],[0.36553723,0.36553723,0.36553723],[0.36553568,0.36553568,0.36553568],[0.3655287,0.3655287,0.3655287],[0.3655038,0.3655038,0.3655038],[0.3654279,0.3654279,0.3654279],[0.36522353,0.36522353,0.36522353],[0.36472595,0.36472595,0.36472595],[0.3636154,0.3636154,0.3636154],[0.36132693,0.36132693,0.36132693],[0.35697365,0.35697365,0.35697365],[0.34937418,0.34937418,0.34937418],[0.3373109,0.3373109,0.3373109],[0.32,0.32,0.32],[0.29746062,0.29746062,0.29746062],[0.27044666,0.27044666,0.27044666],[0.2400381,0.2400381,0.2400381],[0.20725858,0.20725858,0.20725858],[0.17290193,0.17290193,0.17290193],[0.13752115,0.13752115,0.13752115],[0.101480305,0.101480305,0.101480305]],[[0.06589687,0.06589687,0.06589687],[0.10274702,0.10274702,0.10274702],[0.1393581,0.1393581,0.1393581],[0.17559367,0.17559367,0.17559367],[0.21123308,0.21123308,0.21123308],[0.24592173,0.24592173,0.24592173],[0.2791012,0.2791012,0.2791012],[0.30993742,0.30993742,0.30993742],[0.3373109,0.3373109,0.3373109],[0.36,0.36,0.36],[0.37714267,0.37714267,0.37714267],[0.38874853,0.38874853,0.38874853],[0.39576352,0.39576352,0.39576352],[0.3995785,0.3995785,0.3995785],[0.401465,0.401465,0.401465],[0.402318,0.402318,0.402318],[0.4026698,0.4026698,0.4026698],[0.40280062,0.40280062,0.40280062],[0.40284353,0.40284353,0.40284353],[0.40285563,0.40285563,0.40285563],[0.40285838,0.40285838,0.40285838],[0.40285885,0.40285885,0.40285885],[0.4028589,0.4028589,0.4028589],[0.4028589,0.4028589,0.4028589],[0.4028589,0.4028589,0.4028589],[0.4028589,0.4028589,0.4028589],[0.4028589,0.4028589,0.4028589],[0.4028589,0.4028589,0.4028589],[0.4028589,0.4028589,0.4028589],[0.40285885,0.40285885,0.40285885],[0.40285838,0.40285838,0.40285838],[0.40285563,0.40285563,0.40285563],[0.40284353,0.40284353,0.40284353],[0.40280062,0.40280062,0.40280062],[0.4026698,0.4026698,0.4026698],[0.402318,0.402318,0.402318],[0.401465,0.401465,0.401465],[0.3995785,0.3995785,0.3995785],[0.39576352,0.39576352,0.39576352],[0.38874853,0.38874853,0.38874853],[0.37714267,0.37714267,0.37714267],[0.36,0.36,0.36],[0.3373109,0.3373109,0.3373109],[0.30993742,0.30993742,0.30993742],[0.2791012,0.2791012,0.2791012],[0.24592173,0.24592173,0.24592173],[0.21123308,0.21123308,0.21123308],[0.17559367,0.17559367,0.17559367],[0.1393581,0.1393581,0.1393581],[0.10274702,0.10274702,0.10274702]],[[0.06640434,0.06640434,0.06640434],[0.103477,0.103477,0.103477],[0.14042222,0.14042222,0.14042222],[0.17716551,0.17716551,0.17716551],[0.21358395,0.21358395,0.21358395],[0.24947315,0.24947315,0.24947315],[0.28449202,0.28449202,0.28449202],[0.31808287,0.31808287,0.31808287],[0.34937418,0.34937418,0.34937418],[0.37714267,0.37714267,0.37714267],[0.39999998,0.39999998,0.39999998],[0.41695243,0.41695243,0.41695243],[0.4280504,0.4280504,0.4280504],[0.43444133,0.43444133,0.43444133],[0.43771172,0.43771172,0.43771172],[0.4392169,0.4392169,0.4392169],[0.43984264,0.43984264,0.43984264],[0.44007605,0.44007605,0.44007605],[0.44015282,0.44015282,0.44015282],[0.44017434,0.44017434,0.44017434],[0.4401793,0.4401793,0.4401793],[0.44018012,0.44018012,0.44018012],[0.44018018,0.44018018,0.44018018],[0.44018018,0.44018018,0.44018018],[0.44018018,0.44018018,0.44018018],[0.44018018,0.44018018,0.44018018],[0.44018018,0.44018018,0.44018018],[0.44018018,0.44018018,0.44018018],[0.44018018,0.44018018,0.44018018],[0.44018012,0.44018012,0.44018012],[0.4401793,0.4401793,0.4401793],[0.44017434,0.44017434,0.44017434],[0.44015282,0.44015282,0.44015282],[0.44007605,0.44007605,0.44007605],[0.43984264,0.43984264,0.43984264],[0.4392169,0.4392169,0.4392169],[0.43771172,0.43771172,0.43771172],[0.43444133,0.43444133,0.43444133],[0.4280504,0.4280504,0.4280504],[0.41695243,0.41695243,0.41695243],[0.39999998,0.39999998,0.39999998],[0.37714267,0.37714267,0.37714267],[0.34937418,0.34937418,0.34937418],[0.31808287,0.31808287,0.31808287],[0.28449202,0.28449202,0.28449202],[0.24947315,0.24947315,0.24947315],[0.21358395,0.21358395,0.21358395],[0.17716551,0.17716551,0.17716551],[0.14042222,0.14042222,0.14042222],[0.103477,0.103477,0.103477]],[[0.066684365,0.066684365,0.066684365],[0.103880584,0.103880584,0.103880584],[0.14101219,0.14101219,0.14101219],[0.17804116,0.17804116,0.17804116],[0.21490365,0.21490365,0.21490365],[0.2514915,0.2514915,0.2514915],[0.28761846,0.28761846,0.28761846],[0.3229627,0.3229627,0.3229627],[0.35697365,0.35697365,0.35697365],[0.38874853,0.38874853,0.38874853],[0.41695243,0.41695243,0.41695243],[0.44,0.44,0.44],[0.45673555,0.45673555,0.45673555],[0.46726775,0.46726775,0.46726775],[0.47299528,0.47299528,0.47299528],[0.47572267,0.47572267,0.47572267],[0.47687507,0.47687507,0.47687507],[0.47730792,0.47730792,0.47730792],[0.4774505,0.4774505,0.4774505],[0.4774906,0.4774906,0.4774906],[0.47749978,0.47749978,0.47749978],[0.47750133,0.47750133,0.47750133],[0.4775015,0.4775015,0.4775015],[0.4775015,0.4775015,0.4775015],[0.4775015,0.4775015,0.4775015],[0.4775015,0.4775015,0.4775015],[0.4775015,0.4775015,0.4775015],[0.4775015,0.4775015,0.4775015],[0.4775015,0.4775015,0.4775015],[0.47750133,0.47750133,0.47750133],[0.47749978,0.47749978,0.47749978],[0.4774906,0.4774906,0.4774906],[0.4774505,0.4774505,0.4774505],[0.47730792,0.47730792,0.47730792],[0.47687507,0.47687507,0.47687507],[0.47572267,0.47572267,0.47572267],[0.47299528,0.47299528,0.47299528],[0.46726775,0.46726775,0.46726775],[0.45673555,0.45673555,0.45673555],[0.44,0.44,0.44],[0.41695243,0.41695243,0.41695243],[0.38874853,0.38874853,0.38874853],[0.35697365,0.35697365,0.35697365],[0.3229627,0.3229627,0.3229627],[0.28761846,0.28761846,0.28761846],[0.2514915,0.2514915,0.2514915],[0.21490365,0.21490365,0.21490365],[0.17804116,0.17804116,0.17804116],[0.14101219,0.14101219,0.14101219],[0.103880584,0.103880584,0.103880584]],[[0.066832244,0.066832244,0.066832244],[0.10409379,0.10409379,0.10409379],[0.14132446,0.14132446,0.14132446],[0.17850584,0.17850584,0.17850584],[0.21560693,0.21560693,0.21560693],[0.2525748,0.2525748,0.2525748],[0.28931636,0.28931636,0.28931636],[0.32566643,0.32566643,0.32566643],[0.36132693,0.36132693,0.36132693],[0.39576352,0.39576352,0.39576352],[0.4280504,0.4280504,0.4280504],[0.45673555,0.45673555,0.45673555],[0.48000002,0.48000002,0.48000002],[0.4964863,0.4964863,0.4964863],[0.50638616,0.50638616,0.50638616],[0.51141334,0.51141334,0.51141334],[0.51360947,0.51360947,0.51360947],[0.51444626,0.51444626,0.51444626],[0.51472354,0.51472354,0.51472354],[0.5148016,0.5148016,0.5148016],[0.5148194,0.5148194,0.5148194],[0.5148225,0.5148225,0.5148225],[0.51482284,0.51482284,0.51482284],[0.51482284,0.51482284,0.51482284],[0.51482284,0.51482284,0.51482284],[0.51482284,0.51482284,0.51482284],[0.51482284,0.51482284,0.51482284],[0.51482284,0.51482284,0.51482284],[0.51482284,0.51482284,0.51482284],[0.5148225,0.5148225,0.5148225],[0.5148194,0.5148194,0.5148194],[0.5148016,0.5148016,0.5148016],[0.51472354,0.51472354,0.51472354],[0.51444626,0.51444626,0.51444626],[0.51360947,0.51360947,0.51360947],[0.51141334,0.51141334,0.51141334],[0.50638616,0.50638616,0.50638616],[0.4964863,0.4964863,0.4964863],[0.48000002,0.48000002,0.48000002],[0.45673555,0.45673555,0.45673555],[0.4280504,0.4280504,0.4280504],[0.39576352,0.39576352,0.39576352],[0.36132693,0.36132693,0.36132693],[0.32566643,0.32566643,0.32566643],[0.28931636,0.28931636,0.28931636],[0.2525748,0.2525748,0.2525748],[0.21560693,0.21560693,0.21560693],[0.17850584,0.17850584,0.17850584],[0.14132446,0.14132446,0.14132446],[0.10409379,0.10409379,0.10409379]],[[0.06690651,0.06690651,0.06690651],[0.1042009,0.1042009,0.1042009],[0.1414814,0.1414814,0.1414814],[0.17873973,0.17873973,0.17873973],[0.21596181,0.21596181,0.21596181],[0.25312346,0.25312346,0.25312346],[0.29018205,0.29018205,0.29018205],[0.32706022,0.32706022,0.32706022],[0.3636154,0.3636154,0.3636154],[0.3995785,0.3995785,0.3995785],[0.43444133,0.43444133,0.43444133],[0.46726775,0.46726775,0.46726775],[0.4964863,0.4964863,0.4964863],[0.52,0.52,0.52],[0.53619707,0.53619707,0.53619707],[0.5453886,0.5453886,0.5453886],[0.54968387,0.54968387,0.54968387],[0.5513735,0.5513735,0.5513735],[0.5519403,0.5519403,0.5519403],[0.5521004,0.5521004,0.5521004],[0.55213714,0.55213714,0.55213714],[0.5521434,0.5521434,0.5521434],[0.5521441,0.5521441,0.5521441],[0.55214417,0.55214417,0.55214417],[0.55214417,0.55214417,0.55214417],[0.55214417,0.55214417,0.55214417],[0.55214417,0.55214417,0.55214417],[0.55214417,0.55214417,0.55214417],[0.5521441,0.5521441,0.5521441],[0.5521434,0.5521434,0.5521434],[0.55213714,0.55213714,0.55213714],[0.5521004,0.5521004,0.5521004],[0.5519403,0.5519403,0.5519403],[0.5513735,0.5513735,0.5513735],[0.54968387,0.54968387,0.54968387],[0.5453886,0.5453886,0.5453886],[0.53619707,0.53619707,0.53619707],[0.52,0.52,0.52],[0.4964863,0.4964863,0.4964863],[0.46726775,0.46726775,0.46726775],[0.43444133,0.43444133,0.43444133],[0.3995785,0.3995785,0.3995785],[0.3636154,0.3636154,0.3636154],[0.32706022,0.32706022,0.32706022],[0.29018205,0.29018205,0.29018205],[0.25312346,0.25312346,0.25312346],[0.21596181,0.21596181,0.21596181],[0.17873973,0.17873973,0.17873973],[0.1414814,0.1414814,0.1414814],[0.1042009,0.1042009,0.1042009]],[[0.06694168,0.06694168,0.06694168],[0.10425174,0.10425174,0.10425174],[0.14155596,0.14155596,0.14155596],[0.17885089,0.17885089,0.17885089],[0.21613055,0.21613055,0.21613055],[0.25338483,0.25338483,0.25338483],[0.29059547,0.29059547,0.29059547],[0.32772982,0.32772982,0.32772982],[0.36472595,0.36472595,0.36472595],[0.401465,0.401465,0.401465],[0.43771172,0.43771172,0.43771172],[0.47299528,0.47299528,0.47299528],[0.50638616,0.50638616,0.50638616],[0.53619707,0.53619707,0.53619707],[0.56,0.56,0.56],[0.57585764,0.57585764,0.57585764],[0.5842546,0.5842546,0.5842546],[0.5877968,0.5877968,0.5877968],[0.5890206,0.5890206,0.5890206],[0.5893699,0.5893699,0.5893699],[0.58945,0.58945,0.58945],[0.58946383,0.58946383,0.58946383],[0.5894654,0.5894654,0.5894654],[0.5894655,0.5894655,0.5894655],[0.5894655,0.5894655,0.5894655],[0.5894655,0.5894655,0.5894655],[0.5894655,0.5894655,0.5894655],[0.5894655,0.5894655,0.5894655],[0.5894654,0.5894654,0.5894654],[0.58946383,0.58946383,0.58946383],[0.58945,0.58945,0.58945],[0.5893699,0.5893699,0.5893699],[0.5890206,0.5890206,0.5890206],[0.5877968,0.5877968,0.5877968],[0.5842546,0.5842546,0.5842546],[0.57585764,0.57585764,0.57585764],[0.56,0.56,0.56],[0.53619707,0.53619707,0.53619707],[0.50638616,0.50638616,0.50638616],[0.47299528,0.47299528,0.47299528],[0.43771172,0.43771172,0.43771172],[0.401465,0.401465,0.401465],[0.36472595,0.36472595,0.36472595],[0.32772982,0.32772982,0.32772982],[0.29059547,0.29059547,0.29059547],[0.25338483,0.25338483,0.25338483],[0.21613055,0.21613055,0.21613055],[0.17885089,0.17885089,0.17885089],[0.14155596,0.14155596,0.14155596],[0.10425174,0.10425174,0.10425174]],[[0.066957235,0.066957235,0.066957235],[0.10427421,0.10427421,0.10427421],[0.14158893,0.14158893,0.14158893],[0.17890006,0.17890006,0.17890006],[0.21620536,0.21620536,0.21620536],[0.25350076,0.25350076,0.25350076],[0.29077935,0.29077935,0.29077935],[0.32802838,0.32802838,0.32802838],[0.36522353,0.36522353,0.36522353],[0.402318,0.402318,0.402318],[0.4392169,0.4392169,0.4392169],[0.47572267,0.47572267,0.47572267],[0.51141334,0.51141334,0.51141334],[0.5453886,0.5453886,0.5453886],[0.57585764,0.57585764,0.57585764],[0.6,0.6,0.6],[0.61545444,0.61545444,0.61545444],[0.62296087,0.62296087,0.62296087],[0.6257458,0.6257458,0.6257458],[0.62656176,0.62656176,0.62656176],[0.62675035,0.62675035,0.62675035],[0.6267829,0.6267829,0.6267829],[0.6267866,0.6267866,0.6267866],[0.6267868,0.6267868,0.6267868],[0.6267868,0.6267868,0.6267868],[0.6267868,0.6267868,0.6267868],[0.6267868,0.6267868,0.6267868],[0.6267868,0.6267868,0.6267868],[0.6267866,0.6267866,0.6267866],[0.6267829,0.6267829,0.6267829],[0.62675035,0.62675035,0.62675035],[0.62656176,0.62656176,0.62656176],[0.6257458,0.6257458,0.6257458],[0.62296087,0.62296087,0.62296087],[0.61545444,0.61545444,0.61545444],[0.6,0.6,0.6],[0.57585764,0.57585764,0.57585764],[0.5453886,0.5453886,0.5453886],[0.51141334,0.51141334,0.51141334],[0.47572267,0.47572267,0.47572267],[0.4392169,0.4392169,0.4392169],[0.402318,0.402318,0.402318],[0.36522353,0.36522353,0.36522353],[0.32802838,0.32802838,0.32802838],[0.29077935,0.29077935,0.29077935],[0.25350076,0.25350076,0.25350076],[0.21620536,0.21620536,0.21620536],[0.17890006,0.17890006,0.17890006],[0.14158893,0.14158893,0.14158893],[0.10427421,0.10427421,0.10427421]],[[0.06696367,0.06696367,0.06696367],[0.10428339,0.10428339,0.10428339],[0.14160246,0.14160246,0.14160246],[0.17892021,0.17892021,0.17892021],[0.21623588,0.21623588,0.21623588],[0.25354826,0.25354826,0.25354826],[0.29085463,0.29085463,0.29085463],[0.32815063,0.32815063,0.32815063],[0.3654279,0.3654279,0.3654279],[0.4026698,0.4026698,0.4026698],[0.43984264,0.43984264,0.43984264],[0.47687507,0.47687507,0.47687507],[0.51360947,0.51360947,0.51360947],[0.54968387,0.54968387,0.54968387],[0.5842546,0.5842546,0.5842546],[0.61545444,0.61545444,0.61545444],[0.64,0.64,0.64],[0.65496874,0.65496874,0.65496874],[0.6614814,0.6614814,0.6614814],[0.6635301,0.6635301,0.6635301],[0.6640142,0.6640142,0.6640142],[0.664098,0.664098,0.664098],[0.66410756,0.66410756,0.66410756],[0.6641081,0.6641081,0.6641081],[0.66410816,0.66410816,0.66410816],[0.66410816,0.66410816,0.66410816],[0.66410816,0.66410816,0.66410816],[0.6641081,0.6641081,0.6641081],[0.66410756,0.66410756,0.66410756],[0.664098,0.664098,0.664098],[0.6640142,0.6640142,0.6640142],[0.6635301,0.6635301,0.6635301],[0.6614814,0.6614814,0.6614814],[0.65496874,0.65496874,0.65496874],[0.64,0.64,0.64],[0.61545444,0.61545444,0.61545444],[0.5842546,0.5842546,0.5842546],[0.54968387,0.54968387,0.54968387],[0.51360947,0.51360947,0.51360947],[0.47687507,0.47687507,0.47687507],[0.43984264,0.43984264,0.43984264],[0.4026698,0.4026698,0.4026698],[0.3654279,0.3654279,0.3654279],[0.32815063,0.32815063,0.32815063],[0.29085463,0.29085463,0.29085463],[0.25354826,0.25354826,0.25354826],[0.21623588,0.21623588,0.21623588],[0.17892021,0.17892021,0.17892021],[0.14160246,0.14160246,0.14160246],[0.10428339,0.10428339,0.10428339]],[[0.066966,0.066966,0.066966],[0.10428679,0.10428679,0.10428679],[0.1416074,0.1416074,0.1416074],[0.17892766,0.17892766,0.17892766],[0.21624726,0.21624726,0.21624726],[0.25356585,0.25356585,0.25356585],[0.29088253,0.29088253,0.29088253],[0.32819605,0.32819605,0.32819605],[0.3655038,0.3655038,0.3655038],[0.40280062,0.40280062,0.40280062],[0.44007605,0.44007605,0.44007605],[0.47730792,0.47730792,0.47730792],[0.51444626,0.51444626,0.51444626],[0.5513735,0.5513735,0.5513735],[0.5877968,0.5877968,0.5877968],[0.62296087,0.62296087,0.62296087],[0.65496874,0.65496874,0.65496874],[0.68,0.68,0.68],[0.69437426,0.69437426,0.69437426],[0.6997893,0.6997893,0.6997893],[0.701159,0.701159,0.701159],[0.7014003,0.7014003,0.7014003],[0.7014278,0.7014278,0.7014278],[0.7014294,0.7014294,0.7014294],[0.7014295,0.7014295,0.7014295],[0.7014295,0.7014295,0.7014295],[0.7014295,0.7014295,0.7014295],[0.7014294,0.7014294,0.7014294],[0.7014278,0.7014278,0.7014278],[0.7014003,0.7014003,0.7014003],[0.701159,0.701159,0.701159],[0.6997893,0.6997893,0.6997893],[0.69437426,0.69437426,0.69437426],[0.68,0.68,0.68],[0.65496874,0.65496874,0.65496874],[0.62296087,0.62296087,0.62296087],[0.5877968,0.5877968,0.5877968],[0.5513735,0.5513735,0.5513735],[0.51444626,0.51444626,0.51444626],[0.47730792,0.47730792,0.47730792],[0.44007605,0.44007605,0.44007605],[0.40280062,0.40280062,0.40280062],[0.3655038,0.3655038,0.3655038],[0.32819605,0.32819605,0.32819605],[0.29088253,0.29088253,0.29088253],[0.25356585,0.25356585,0.25356585],[0.21624726,0.21624726,0.21624726],[0.17892766,0.17892766,0.17892766],[0.1416074,0.1416074,0.1416074],[0.10428679,0.10428679,0.10428679]],[[0.06696677,0.06696677,0.06696677],[0.10428798,0.10428798,0.10428798],[0.14160907,0.14160907,0.14160907],[0.1789301,0.1789301,0.1789301],[0.21625096,0.21625096,0.21625096],[0.25357157,0.25357157,0.25357157],[0.2908917,0.2908917,0.2908917],[0.32821095,0.32821095,0.32821095],[0.3655287,0.3655287,0.3655287],[0.40284353,0.40284353,0.40284353],[0.44015282,0.44015282,0.44015282],[0.4774505,0.4774505,0.4774505],[0.51472354,0.51472354,0.51472354],[0.5519403,0.5519403,0.5519403],[0.5890206,0.5890206,0.5890206],[0.6257458,0.6257458,0.6257458],[0.6614814,0.6614814,0.6614814],[0.69437426,0.69437426,0.69437426],[0.72,0.72,0.72],[0.7336339,0.7336339,0.7336339],[0.73786134,0.73786134,0.73786134],[0.73865396,0.73865396,0.73865396],[0.73874533,0.73874533,0.73874533],[0.7387507,0.7387507,0.7387507],[0.73875076,0.73875076,0.73875076],[0.73875076,0.73875076,0.73875076],[0.73875076,0.73875076,0.73875076],[0.7387507,0.7387507,0.7387507],[0.73874533,0.73874533,0.73874533],[0.73865396,0.73865396,0.73865396],[0.73786134,0.73786134,0.73786134],[0.7336339,0.7336339,0.7336339],[0.72,0.72,0.72],[0.69437426,0.69437426,0.69437426],[0.6614814,0.6614814,0.6614814],[0.6257458,0.6257458,0.6257458],[0.5890206,0.5890206,0.5890206],[0.5519403,0.5519403,0.5519403],[0.51472354,0.51472354,0.51472354],[0.4774505,0.4774505,0.4774505],[0.44015282,0.44015282,0.44015282],[0.40284353,0.40284353,0.40284353],[0.3655287,0.3655287,0.3655287],[0.32821095,0.32821095,0.32821095],[0.2908917,0.2908917,0.2908917],[0.25357157,0.25357157,0.25357157],[0.21625096,0.21625096,0.21625096],[0.1789301,0.1789301,0.1789301],[0.14160907,0.14160907,0.14160907],[0.10428798,0.10428798,0.10428798]],[[0.06696701,0.06696701,0.06696701],[0.10428822,0.10428822,0.10428822],[0.14160955,0.14160955,0.14160955],[0.17893082,0.17893082,0.17893082],[0.21625197,0.21625197,0.21625197],[0.25357318,0.25357318,0.25357318],[0.29089427,0.29089427,0.29089427],[0.32821512,0.32821512,0.32821512],[0.36553568,0.36553568,0.36553568],[0.40285563,0.40285563,0.40285563],[0.44017434,0.44017434,0.44017434],[0.4774906,0.4774906,0.4774906],[0.5148016,0.5148016,0.5148016],[0.5521004,0.5521004,0.5521004],[0.5893699,0.5893699,0.5893699],[0.62656176,0.62656176,0.62656176],[0.6635301,0.6635301,0.6635301],[0.6997893,0.6997893,0.6997893],[0.7336339,0.7336339,0.7336339],[0.76,0.76,0.76],[0.7726943,0.7726943,0.7726943],[0.77568674,0.77568674,0.77568674],[0.7760502,0.7760502,0.7760502],[0.77607167,0.77607167,0.77607167],[0.7760721,0.7760721,0.7760721],[0.7760721,0.7760721,0.7760721],[0.7760721,0.7760721,0.7760721],[0.77607167,0.77607167,0.77607167],[0.7760502,0.7760502,0.7760502],[0.77568674,0.77568674,0.77568674],[0.7726943,0.7726943,0.7726943],[0.76,0.76,0.76],[0.7336339,0.7336339,0.7336339],[0.6997893,0.6997893,0.6997893],[0.6635301,0.6635301,0.6635301],[0.62656176,0.62656176,0.62656176],[0.5893699,0.5893699,0.5893699],[0.5521004,0.5521004,0.5521004],[0.5148016,0.5148016,0.5148016],[0.4774906,0.4774906,0.4774906],[0.44017434,0.44017434,0.44017434],[0.40285563,0.40285563,0.40285563],[0.36553568,0.36553568,0.36553568],[0.32821512,0.32821512,0.32821512],[0.29089427,0.29089427,0.29089427],[0.25357318,0.25357318,0.25357318],[0.21625197,0.21625197,0.21625197],[0.17893082,0.17893082,0.17893082],[0.14160955,0.14160955,0.14160955],[0.10428822,0.10428822,0.10428822]],[[0.06696701,0.06696701,0.06696701],[0.10428834,0.10428834,0.10428834],[0.14160961,0.14160961,0.14160961],[0.17893094,0.17893094,0.17893094],[0.21625227,0.21625227,0.21625227],[0.2535736,0.2535736,0.2535736],[0.2908948,0.2908948,0.2908948],[0.32821608,0.32821608,0.32821608],[0.36553723,0.36553723,0.36553723],[0.40285838,0.40285838,0.40285838],[0.4401793,0.4401793,0.4401793],[0.47749978,0.47749978,0.47749978],[0.5148194,0.5148194,0.5148194],[0.55213714,0.55213714,0.55213714],[0.58945,0.58945,0.58945],[0.62675035,0.62675035,0.62675035],[0.6640142,0.6640142,0.6640142],[0.701159,0.701159,0.701159],[0.73786134,0.73786134,0.73786134],[0.7726943,0.7726943,0.7726943],[0.8,0.8,0.8],[0.81148046,0.81148046,0.81148046],[0.8132809,0.8132809,0.8132809],[0.81339145,0.81339145,0.81339145],[0.8133934,0.8133934,0.8133934],[0.8133934,0.8133934,0.8133934],[0.8133934,0.8133934,0.8133934],[0.81339145,0.81339145,0.81339145],[0.8132809,0.8132809,0.8132809],[0.81148046,0.81148046,0.81148046],[0.8,0.8,0.8],[0.7726943,0.7726943,0.7726943],[0.73786134,0.73786134,0.73786134],[0.701159,0.701159,0.701159],[0.6640142,0.6640142,0.6640142],[0.62675035,0.62675035,0.62675035],[0.58945,0.58945,0.58945],[0.55213714,0.55213714,0.55213714],[0.5148194,0.5148194,0.5148194],[0.47749978,0.47749978,0.47749978],[0.4401793,0.4401793,0.4401793],[0.40285838,0.40285838,0.40285838],[0.36553723,0.36553723,0.36553723],[0.32821608,0.32821608,0.32821608],[0.2908948,0.2908948,0.2908948],[0.2535736,0.2535736,0.2535736],[0.21625227,0.21625227,0.21625227],[0.17893094,0.17893094,0.17893094],[0.14160961,0.14160961,0.14160961],[0.10428834,0.10428834,0.10428834]],[[0.06696701,0.06696701,0.06696701],[0.10428834,0.10428834,0.10428834],[0.14160961,0.14160961,0.14160961],[0.17893094,0.17893094,0.17893094],[0.21625227,0.21625227,0.21625227],[0.2535736,0.2535736,0.2535736],[0.29089493,0.29089493,0.29089493],[0.3282162,0.3282162,0.3282162],[0.36553752,0.36553752,0.36553752],[0.40285885,0.40285885,0.40285885],[0.44018012,0.44018012,0.44018012],[0.47750133,0.47750133,0.47750133],[0.5148225,0.5148225,0.5148225],[0.5521434,0.5521434,0.5521434],[0.58946383,0.58946383,0.58946383],[0.6267829,0.6267829,0.6267829],[0.664098,0.664098,0.664098],[0.7014003,0.7014003,0.7014003],[0.73865396,0.73865396,0.73865396],[0.77568674,0.77568674,0.77568674],[0.81148046,0.81148046,0.81148046],[0.84000003,0.84000003,0.84000003],[0.84989464,0.84989464,0.84989464],[0.85070014,0.85070014,0.85070014],[0.8507147,0.8507147,0.8507147],[0.85071474,0.85071474,0.85071474],[0.8507147,0.8507147,0.8507147],[0.85070014,0.85070014,0.85070014],[0.84989464,0.84989464,0.84989464],[0.84000003,0.84000003,0.84000003],[0.81148046,0.81148046,0.81148046],[0.77568674,0.77568674,0.77568674],[0.73865396,0.73865396,0.73865396],[0.7014003,0.7014003,0.7014003],[0.664098,0.664098,0.664098],[0.6267829,0.6267829,0.6267829],[0.58946383,0.58946383,0.58946383],[0.5521434,0.5521434,0.5521434],[0.5148225,0.5148225,0.5148225],[0.47750133,0.47750133,0.47750133],[0.44018012,0.44018012,0.44018012],[0.40285885,0.40285885,0.40285885],[0.36553752,0.36553752,0.36553752],[0.3282162,0.3282162,0.3282162],[0.29089493,0.29089493,0.29089493],[0.2535736,0.2535736,0.2535736],[0.21625227,0.21625227,0.21625227],[0.17893094,0.17893094,0.17893094],[0.14160961,0.14160961,0.14160961],[0.10428834,0.10428834,0.10428834]],[[0.06696701,0.06696701,0.06696701],[0.10428834,0.10428834,0.10428834],[0.14160961,0.14160961,0.14160961],[0.17893094,0.17893094,0.17893094],[0.21625227,0.21625227,0.21625227],[0.25357366,0.25357366,0.25357366],[0.290895,0.290895,0.290895],[0.3282163,0.3282163,0.3282163],[0.36553758,0.36553758,0.36553758],[0.4028589,0.4028589,0.4028589],[0.44018018,0.44018018,0.44018018],[0.4775015,0.4775015,0.4775015],[0.51482284,0.51482284,0.51482284],[0.5521441,0.5521441,0.5521441],[0.5894654,0.5894654,0.5894654],[0.6267866,0.6267866,0.6267866],[0.66410756,0.66410756,0.66410756],[0.7014278,0.7014278,0.7014278],[0.73874533,0.73874533,0.73874533],[0.7760502,0.7760502,0.7760502],[0.8132809,0.8132809,0.8132809],[0.84989464,0.84989464,0.84989464],[0.88,0.88,0.88],[0.8878434,0.8878434,0.8878434],[0.8880359,0.8880359,0.8880359],[0.8880361,0.8880361,0.8880361],[0.8880359,0.8880359,0.8880359],[0.8878434,0.8878434,0.8878434],[0.88,0.88,0.88],[0.84989464,0.84989464,0.84989464],[0.8132809,0.8132809,0.8132809],[0.7760502,0.7760502,0.7760502],[0.73874533,0.73874533,0.73874533],[0.7014278,0.7014278,0.7014278],[0.66410756,0.66410756,0.66410756],[0.6267866,0.6267866,0.6267866],[0.5894654,0.5894654,0.5894654],[0.5521441,0.5521441,0.5521441],[0.51482284,0.51482284,0.51482284],[0.4775015,0.4775015,0.4775015],[0.44018018,0.44018018,0.44018018],[0.4028589,0.4028589,0.4028589],[0.36553758,0.36553758,0.36553758],[0.3282163,0.3282163,0.3282163],[0.290895,0.290895,0.290895],[0.25357366,0.25357366,0.25357366],[0.21625227,0.21625227,0.21625227],[0.17893094,0.17893094,0.17893094],[0.14160961,0.14160961,0.14160961],[0.10428834,0.10428834,0.10428834]],[[0.06696701,0.06696701,0.06696701],[0.10428834,0.10428834,0.10428834],[0.14160961,0.14160961,0.14160961],[0.17893094,0.17893094,0.17893094],[0.21625227,0.21625227,0.21625227],[0.25357366,0.25357366,0.25357366],[0.290895,0.290895,0.290895],[0.3282163,0.3282163,0.3282163],[0.36553758,0.36553758,0.36553758],[0.4028589,0.4028589,0.4028589],[0.44018018,0.44018018,0.44018018],[0.4775015,0.4775015,0.4775015],[0.51482284,0.51482284,0.51482284],[0.55214417,0.55214417,0.55214417],[0.5894655,0.5894655,0.5894655],[0.6267868,0.6267868,0.6267868],[0.6641081,0.6641081,0.6641081],[0.7014294,0.7014294,0.7014294],[0.7387507,0.7387507,0.7387507],[0.77607167,0.77607167,0.77607167],[0.81339145,0.81339145,0.81339145],[0.85070014,0.85070014,0.85070014],[0.8878434,0.8878434,0.8878434],[0.92,0.92,0.92],[0.92535007,0.92535007,0.92535007],[0.92535734,0.92535734,0.92535734],[0.92535007,0.92535007,0.92535007],[0.92,0.92,0.92],[0.8878434,0.8878434,0.8878434],[0.85070014,0.85070014,0.85070014],[0.81339145,0.81339145,0.81339145],[0.77607167,0.77607167,0.77607167],[0.7387507,0.7387507,0.7387507],[0.7014294,0.7014294,0.7014294],[0.6641081,0.6641081,0.6641081],[0.6267868,0.6267868,0.6267868],[0.5894655,0.5894655,0.5894655],[0.55214417,0.55214417,0.55214417],[0.51482284,0.51482284,0.51482284],[0.4775015,0.4775015,0.4775015],[0.44018018,0.44018018,0.44018018],[0.4028589,0.4028589,0.4028589],[0.36553758,0.36553758,0.36553758],[0.3282163,0.3282163,0.3282163],[0.290895,0.290895,0.290895],[0.25357366,0.25357366,0.25357366],[0.21625227,0.21625227,0.21625227],[0.17893094,0.17893094,0.17893094],[0.14160961,0.14160961,0.14160961],[0.10428834,0.10428834,0.10428834]],[[0.06696701,0.06696701,0.06696701],[0.10428834,0.10428834,0.10428834],[0.14160961,0.14160961,0.14160961],[0.17893094,0.17893094,0.17893094],[0.21625227,0.21625227,0.21625227],[0.25357366,0.25357366,0.25357366],[0.290895,0.290895,0.290895],[0.3282163,0.3282163,0.3282163],[0.36553758,0.36553758,0.36553758],[0.4028589,0.4028589,0.4028589],[0.44018018,0.44018018,0.44018018],[0.4775015,0.4775015,0.4775015],[0.51482284,0.51482284,0.51482284],[0.55214417,0.55214417,0.55214417],[0.5894655,0.5894655,0.5894655],[0.6267868,0.6267868,0.6267868],[0.66410816,0.66410816,0.66410816],[0.7014295,0.7014295,0.7014295],[0.73875076,0.73875076,0.73875076],[0.7760721,0.7760721,0.7760721],[0.8133934,0.8133934,0.8133934],[0.8507147,0.8507147,0.8507147],[0.8880359,0.8880359,0.8880359],[0.92535007,0.92535007,0.92535007],[0.96,0.96,0.96],[0.9626787,0.9626787,0.9626787],[0.96,0.96,0.96],[0.92535007,0.92535007,0.92535007],[0.8880359,0.8880359,0.8880359],[0.8507147,0.8507147,0.8507147],[0.8133934,0.8133934,0.8133934],[0.7760721,0.7760721,0.7760721],[0.73875076,0.73875076,0.73875076],[0.7014295,0.7014295,0.7014295],[0.66410816,0.66410816,0.66410816],[0.6267868,0.6267868,0.6267868],[0.5894655,0.5894655,0.5894655],[0.55214417,0.55214417,0.55214417],[0.51482284,0.51482284,0.51482284],[0.4775015,0.4775015,0.4775015],[0.44018018,0.44018018,0.44018018],[0.4028589,0.4028589,0.4028589],[0.36553758,0.36553758,0.36553758],[0.3282163,0.3282163,0.3282163],[0.290895,0.290895,0.290895],[0.25357366,0.25357366,0.25357366],[0.21625227,0.21625227,0.21625227],[0.17893094,0.17893094,0.17893094],[0.14160961,0.14160961,0.14160961],[0.10428834,0.10428834,0.10428834]],[[0.06696701,0.06696701,0.06696701],[0.10428834,0.10428834,0.10428834],[0.14160961,0.14160961,0.14160961],[0.17893094,0.17893094,0.17893094],[0.21625227,0.21625227,0.21625227],[0.25357366,0.25357366,0.25357366],[0.290895,0.290895,0.290895],[0.3282163,0.3282163,0.3282163],[0.36553758,0.36553758,0.36553758],[0.4028589,0.4028589,0.4028589],[0.44018018,0.44018018,0.44018018],[0.4775015,0.4775015,0.4775015],[0.51482284,0.51482284,0.51482284],[0.55214417,0.55214417,0.55214417],[0.5894655,0.5894655,0.5894655],[0.6267868,0.6267868,0.6267868],[0.66410816,0.66410816,0.66410816],[0.7014295,0.7014295,0.7014295],[0.73875076,0.73875076,0.73875076],[0.7760721,0.7760721,0.7760721],[0.8133934,0.8133934,0.8133934],[0.85071474,0.85071474,0.85071474],[0.8880361,0.8880361,0.8880361],[0.92535734,0.92535734,0.92535734],[0.9626787,0.9626787,0.9626787],[1.0,1.0,1.0],[0.9626787,0.9626787,0.9626787],[0.92535734,0.92535734,0.92535734],[0.8880361,0.8880361,0.8880361],[0.85071474,0.85071474,0.85071474],[0.8133934,0.8133934,0.8133934],[0.7760721,0.7760721,0.7760721],[0.73875076,0.73875076,0.73875076],[0.7014295,0.7014295,0.7014295],[0.66410816,0.66410816,0.66410816],[0.6267868,0.6267868,0.6267868],[0.5894655,0.5894655,0.5894655],[0.55214417,0.55214417,0.55214417],[0.51482284,0.51482284,0.51482284],[0.4775015,0.4775015,0.4775015],[0.44018018,0.44018018,0.44018018],[0.4028589,0.4028589,0.4028589],[0.36553758,0.36553758,0.36553758],[0.3282163,0.3282163,0.3282163],[0.290895,0.290895,0.290895],[0.25357366,0.25357366,0.25357366],[0.21625227,0.21625227,0.21625227],[0.17893094,0.17893094,0.17893094],[0.14160961,0.14160961,0.14160961],[0.10428834,0.10428834,0.10428834]],[[0.06696701,0.06696701,0.06696701],[0.10428834,0.10428834,0.10428834],[0.14160961,0.14160961,0.14160961],[0.17893094,0.17893094,0.17893094],[0.21625227,0.21625227,0.21625227],[0.25357366,0.25357366,0.25357366],[0.290895,0.290895,0.290895],[0.3282163,0.3282163,0.3282163],[0.36553758,0.36553758,0.36553758],[0.4028589,0.4028589,0.4028589],[0.44018018,0.44018018,0.44018018],[0.4775015,0.4775015,0.4775015],[0.51482284,0.51482284,0.51482284],[0.55214417,0.55214417,0.55214417],[0.5894655,0.5894655,0.5894655],[0.6267868,0.6267868,0.6267868],[0.66410816,0.66410816,0.66410816],[0.7014295,0.7014295,0.7014295],[0.73875076,0.73875076,0.73875076],[0.7760721,0.7760721,0.7760721],[0.8133934,0.8133934,0.8133934],[0.8507147,0.8507147,0.8507147],[0.8880359,0.8880359,0.8880359],[0.92535007,0.92535007,0.92535007],[0.96,0.96,0.96],[0.9626787,0.9626787,0.9626787],[0.96,0.96,0.96],[0.92535007,0.92535007,0.92535007],[0.8880359,0.8880359,0.8880359],[0.8507147,0.8507147,0.8507147],[0.8133934,0.8133934,0.8133934],[0.7760721,0.7760721,0.7760721],[0.73875076,0.73875076,0.73875076],[0.7014295,0.7014295,0.7014295],[0.66410816,0.66410816,0.66410816],[0.6267868,0.6267868,0.6267868],[0.5894655,0.5894655,0.5894655],[0.55214417,0.55214417,0.55214417],[0.51482284,0.51482284,0.51482284],[0.4775015,0.4775015,0.4775015],[0.44018018,0.44018018,0.44018018],[0.4028589,0.4028589,0.4028589],[0.36553758,0.36553758,0.36553758],[0.3282163,0.3282163,0.3282163],[0.290895,0.290895,0.290895],[0.25357366,0.25357366,0.25357366],[0.21625227,0.21625227,0.21625227],[0.17893094,0.17893094,0.17893094],[0.14160961,0.14160961,0.14160961],[0.10428834,0.10428834,0.10428834]],[[0.06696701,0.06696701,0.06696701],[0.10428834,0.10428834,0.10428834],[0.14160961,0.14160961,0.14160961],[0.17893094,0.17893094,0.17893094],[0.21625227,0.21625227,0.21625227],[0.25357366,0.25357366,0.25357366],[0.290895,0.290895,0.290895],[0.3282163,0.3282163,0.3282163],[0.36553758,0.36553758,0.36553758],[0.4028589,0.4028589,0.4028589],[0.44018018,0.44018018,0.44018018],[0.4775015,0.4775015,0.4775015],[0.51482284,0.51482284,0.51482284],[0.55214417,0.55214417,0.55214417],[0.5894655,0.5894655,0.5894655],[0.6267868,0.6267868,0.6267868],[0.6641081,0.6641081,0.6641081],[0.7014294,0.7014294,0.7014294],[0.7387507,0.7387507,0.7387507],[0.77607167,0.77607167,0.77607167],[0.81339145,0.81339145,0.81339145],[0.85070014,0.85070014,0.85070014],[0.8878434,0.8878434,0.8878434],[0.92,0.92,0.92],[0.92535007,0.92535007,0.92535007],[0.92535734,0.92535734,0.92535734],[0.92535007,0.92535007,0.92535007],[0.92,0.92,0.92],[0.8878434,0.8878434,0.8878434],[0.85070014,0.85070014,0.85070014],[0.81339145,0.81339145,0.81339145],[0.77607167,0.77607167,0.77607167],[0.7387507,0.7387507,0.7387507],[0.7014294,0.7014294,0.7014294],[0.6641081,0.6641081,0.6641081],[0.6267868,0.6267868,0.6267868],[0.5894655,0.5894655,0.5894655],[0.55214417,0.55214417,0.55214417],[0.51482284,0.51482284,0.51482284],[0.4775015,0.4775015,0.4775015],[0.44018018,0.44018018,0.44018018],[0.4028589,0.4028589,0.4028589],[0.36553758,0.36553758,0.36553758],[0.3282163,0.3282163,0.3282163],[0.290895,0.290895,0.290895],[0.25357366,0.25357366,0.25357366],[0.21625227,0.21625227,0.21625227],[0.17893094,0.17893094,0.17893094],[0.14160961,0.14160961,0.14160961],[0.10428834,0.10428834,0.10428834]],[[0.06696701,0.06696701,0.06696701],[0.10428834,0.10428834,0.10428834],[0.14160961,0.14160961,0.14160961],[0.17893094,0.17893094,0.17893094],[0.21625227,0.21625227,0.21625227],[0.25357366,0.25357366,0.25357366],[0.290895,0.290895,0.290895],[0.3282163,0.3282163,0.3282163],[0.36553758,0.36553758,0.36553758],[0.4028589,0.4028589,0.4028589],[0.44018018,0.44018018,0.44018018],[0.4775015,0.4775015,0.4775015],[0.51482284,0.51482284,0.51482284],[0.5521441,0.5521441,0.5521441],[0.5894654,0.5894654,0.5894654],[0.6267866,0.6267866,0.6267866],[0.66410756,0.66410756,0.66410756],[0.7014278,0.7014278,0.7014278],[0.73874533,0.73874533,0.73874533],[0.7760502,0.7760502,0.7760502],[0.8132809,0.8132809,0.8132809],[0.84989464,0.84989464,0.84989464],[0.88,0.88,0.88],[0.8878434,0.8878434,0.8878434],[0.8880359,0.8880359,0.8880359],[0.8880361,0.8880361,0.8880361],[0.8880359,0.8880359,0.8880359],[0.8878434,0.8878434,0.8878434],[0.88,0.88,0.88],[0.84989464,0.84989464,0.84989464],[0.8132809,0.8132809,0.8132809],[0.7760502,0.7760502,0.7760502],[0.73874533,0.73874533,0.73874533],[0.7014278,0.7014278,0.7014278],[0.66410756,0.66410756,0.66410756],[0.6267866,0.6267866,0.6267866],[0.5894654,0.5894654,0.5894654],[0.5521441,0.5521441,0.5521441],[0.51482284,0.51482284,0.51482284],[0.4775015,0.4775015,0.4775015],[0.44018018,0.44018018,0.44018018],[0.4028589,0.4028589,0.4028589],[0.36553758,0.36553758,0.36553758],[0.3282163,0.3282163,0.3282163],[0.290895,0.290895,0.290895],[0.25357366,0.25357366,0.25357366],[0.21625227,0.21625227,0.21625227],[0.17893094,0.17893094,0.17893094],[0.14160961,0.14160961,0.14160961],[0.10428834,0.10428834,0.10428834]],[[0.06696701,0.06696701,0.06696701],[0.10428834,0.10428834,0.10428834],[0.14160961,0.14160961,0.14160961],[0.17893094,0.17893094,0.17893094],[0.21625227,0.21625227,0.21625227],[0.2535736,0.2535736,0.2535736],[0.29089493,0.29089493,0.29089493],[0.3282162,0.3282162,0.3282162],[0.36553752,0.36553752,0.36553752],[0.40285885,0.40285885,0.40285885],[0.44018012,0.44018012,0.44018012],[0.47750133,0.47750133,0.47750133],[0.5148225,0.5148225,0.5148225],[0.5521434,0.5521434,0.5521434],[0.58946383,0.58946383,0.58946383],[0.6267829,0.6267829,0.6267829],[0.664098,0.664098,0.664098],[0.7014003,0.7014003,0.7014003],[0.73865396,0.73865396,0.73865396],[0.77568674,0.77568674,0.77568674],[0.81148046,0.81148046,0.81148046],[0.84000003,0.84000003,0.84000003],[0.84989464,0.84989464,0.84989464],[0.85070014,0.85070014,0.85070014],[0.8507147,0.8507147,0.8507147],[0.85071474,0.85071474,0.85071474],[0.8507147,0.8507147,0.8507147],[0.85070014,0.85070014,0.85070014],[0.84989464,0.84989464,0.84989464],[0.84000003,0.84000003,0.84000003],[0.81148046,0.81148046,0.81148046],[0.77568674,0.77568674,0.77568674],[0.73865396,0.73865396,0.73865396],[0.7014003,0.7014003,0.7014003],[0.664098,0.664098,0.664098],[0.6267829,0.6267829,0.6267829],[0.58946383,0.58946383,0.58946383],[0.5521434,0.5521434,0.5521434],[0.5148225,0.5148225,0.5148225],[0.47750133,0.47750133,0.47750133],[0.44018012,0.44018012,0.44018012],[0.40285885,0.40285885,0.40285885],[0.36553752,0.36553752,0.36553752],[0.3282162,0.3282162,0.3282162],[0.29089493,0.29089493,0.29089493],[0.2535736,0.2535736,0.2535736],[0.21625227,0.21625227,0.21625227],[0.17893094,0.17893094,0.17893094],[0.14160961,0.14160961,0.14160961],[0.10428834,0.10428834,0.10428834]],[[0.06696701,0.06696701,0.06696701],[0.10428834,0.10428834,0.10428834],[0.14160961,0.14160961,0.14160961],[0.17893094,0.17893094,0.17893094],[0.21625227,0.21625227,0.21625227],[0.2535736,0.2535736,0.2535736],[0.2908948,0.2908948,0.2908948],[0.32821608,0.32821608,0.32821608],[0.36553723,0.36553723,0.36553723],[0.40285838,0.40285838,0.40285838],[0.4401793,0.4401793,0.4401793],[0.47749978,0.47749978,0.47749978],[0.5148194,0.5148194,0.5148194],[0.55213714,0.55213714,0.55213714],[0.58945,0.58945,0.58945],[0.62675035,0.62675035,0.62675035],[0.6640142,0.6640142,0.6640142],[0.701159,0.701159,0.701159],[0.73786134,0.73786134,0.73786134],[0.7726943,0.7726943,0.7726943],[0.8,0.8,0.8],[0.81148046,0.81148046,0.81148046],[0.8132809,0.8132809,0.8132809],[0.81339145,0.81339145,0.81339145],[0.8133934,0.8133934,0.8133934],[0.8133934,0.8133934,0.8133934],[0.8133934,0.8133934,0.8133934],[0.81339145,0.81339145,0.81339145],[0.8132809,0.8132809,0.8132809],[0.81148046,0.81148046,0.81148046],[0.8,0.8,0.8],[0.7726943,0.7726943,0.7726943],[0.73786134,0.73786134,0.73786134],[0.701159,0.701159,0.701159],[0.6640142,0.6640142,0.6640142],[0.62675035,0.62675035,0.62675035],[0.58945,0.58945,0.58945],[0.55213714,0.55213714,0.55213714],[0.5148194,0.5148194,0.5148194],[0.47749978,0.47749978,0.47749978],[0.4401793,0.4401793,0.4401793],[0.40285838,0.40285838,0.40285838],[0.36553723,0.36553723,0.36553723],[0.32821608,0.32821608,0.32821608],[0.2908948,0.2908948,0.2908948],[0.2535736,0.2535736,0.2535736],[0.21625227,0.21625227,0.21625227],[0.17893094,0.17893094,0.17893094],[0.14160961,0.14160961,0.14160961],[0.10428834,0.10428834,0.10428834]],[[0.06696701,0.06696701,0.06696701],[0.10428822,0.10428822,0.10428822],[0.14160955,0.14160955,0.14160955],[0.17893082,0.17893082,0.17893082],[0.21625197,0.21625197,0.21625197],[0.25357318,0.25357318,0.25357318],[0.29089427,0.29089427,0.29089427],[0.32821512,0.32821512,0.32821512],[0.36553568,0.36553568,0.36553568],[0.40285563,0.40285563,0.40285563],[0.44017434,0.44017434,0.44017434],[0.4774906,0.4774906,0.4774906],[0.5148016,0.5148016,0.5148016],[0.5521004,0.5521004,0.5521004],[0.5893699,0.5893699,0.5893699],[0.62656176,0.62656176,0.62656176],[0.6635301,0.6635301,0.6635301],[0.6997893,0.6997893,0.6997893],[0.7336339,0.7336339,0.7336339],[0.76,0.76,0.76],[0.7726943,0.7726943,0.7726943],[0.77568674,0.77568674,0.77568674],[0.7760502,0.7760502,0.7760502],[0.77607167,0.77607167,0.77607167],[0.7760721,0.7760721,0.7760721],[0.7760721,0.7760721,0.7760721],[0.7760721,0.7760721,0.7760721],[0.77607167,0.77607167,0.77607167],[0.7760502,0.7760502,0.7760502],[0.77568674,0.77568674,0.77568674],[0.7726943,0.7726943,0.7726943],[0.76,0.76,0.76],[0.7336339,0.7336339,0.7336339],[0.6997893,0.6997893,0.6997893],[0.6635301,0.6635301,0.6635301],[0.62656176,0.62656176,0.62656176],[0.5893699,0.5893699,0.5893699],[0.5521004,0.5521004,0.5521004],[0.5148016,0.5148016,0.5148016],[0.4774906,0.4774906,0.4774906],[0.44017434,0.44017434,0.44017434],[0.40285563,0.40285563,0.40285563],[0.36553568,0.36553568,0.36553568],[0.32821512,0.32821512,0.32821512],[0.29089427,0.29089427,0.29089427],[0.25357318,0.25357318,0.25357318],[0.21625197,0.21625197,0.21625197],[0.17893082,0.17893082,0.17893082],[0.14160955,0.14160955,0.14160955],[0.10428822,0.10428822,0.10428822]],[[0.06696677,0.06696677,0.06696677],[0.10428798,0.10428798,0.10428798],[0.14160907,0.14160907,0.14160907],[0.1789301,0.1789301,0.1789301],[0.21625096,0.21625096,0.21625096],[0.25357157,0.25357157,0.25357157],[0.2908917,0.2908917,0.2908917],[0.32821095,0.32821095,0.32821095],[0.3655287,0.3655287,0.3655287],[0.40284353,0.40284353,0.40284353],[0.44015282,0.44015282,0.44015282],[0.4774505,0.4774505,0.4774505],[0.51472354,0.51472354,0.51472354],[0.5519403,0.5519403,0.5519403],[0.5890206,0.5890206,0.5890206],[0.6257458,0.6257458,0.6257458],[0.6614814,0.6614814,0.6614814],[0.69437426,0.69437426,0.69437426],[0.72,0.72,0.72],[0.7336339,0.7336339,0.7336339],[0.73786134,0.73786134,0.73786134],[0.73865396,0.73865396,0.73865396],[0.73874533,0.73874533,0.73874533],[0.7387507,0.7387507,0.7387507],[0.73875076,0.73875076,0.73875076],[0.73875076,0.73875076,0.73875076],[0.73875076,0.73875076,0.73875076],[0.7387507,0.7387507,0.7387507],[0.73874533,0.73874533,0.73874533],[0.73865396,0.73865396,0.73865396],[0.73786134,0.73786134,0.73786134],[0.7336339,0.7336339,0.7336339],[0.72,0.72,0.72],[0.69437426,0.69437426,0.69437426],[0.6614814,0.6614814,0.6614814],[0.6257458,0.6257458,0.6257458],[0.5890206,0.5890206,0.5890206],[0.5519403,0.5519403,0.5519403],[0.51472354,0.51472354,0.51472354],[0.4774505,0.4774505,0.4774505],[0.44015282,0.44015282,0.44015282],[0.40284353,0.40284353,0.40284353],[0.3655287,0.3655287,0.3655287],[0.32821095,0.32821095,0.32821095],[0.2908917,0.2908917,0.2908917],[0.25357157,0.25357157,0.25357157],[0.21625096,0.21625096,0.21625096],[0.1789301,0.1789301,0.1789301],[0.14160907,0.14160907,0.14160907],[0.10428798,0.10428798,0.10428798]],[[0.066966,0.066966,0.066966],[0.10428679,0.10428679,0.10428679],[0.1416074,0.1416074,0.1416074],[0.17892766,0.17892766,0.17892766],[0.21624726,0.21624726,0.21624726],[0.25356585,0.25356585,0.25356585],[0.29088253,0.29088253,0.29088253],[0.32819605,0.32819605,0.32819605],[0.3655038,0.3655038,0.3655038],[0.40280062,0.40280062,0.40280062],[0.44007605,0.44007605,0.44007605],[0.47730792,0.47730792,0.47730792],[0.51444626,0.51444626,0.51444626],[0.5513735,0.5513735,0.5513735],[0.5877968,0.5877968,0.5877968],[0.62296087,0.62296087,0.62296087],[0.65496874,0.65496874,0.65496874],[0.68,0.68,0.68],[0.69437426,0.69437426,0.69437426],[0.6997893,0.6997893,0.6997893],[0.701159,0.701159,0.701159],[0.7014003,0.7014003,0.7014003],[0.7014278,0.7014278,0.7014278],[0.7014294,0.7014294,0.7014294],[0.7014295,0.7014295,0.7014295],[0.7014295,0.7014295,0.7014295],[0.7014295,0.7014295,0.7014295],[0.7014294,0.7014294,0.7014294],[0.7014278,0.7014278,0.7014278],[0.7014003,0.7014003,0.7014003],[0.701159,0.701159,0.701159],[0.6997893,0.6997893,0.6997893],[0.69437426,0.69437426,0.69437426],[0.68,0.68,0.68],[0.65496874,0.65496874,0.65496874],[0.62296087,0.62296087,0.62296087],[0.5877968,0.5877968,0.5877968],[0.5513735,0.5513735,0.5513735],[0.51444626,0.51444626,0.51444626],[0.47730792,0.47730792,0.47730792],[0.44007605,0.44007605,0.44007605],[0.40280062,0.40280062,0.40280062],[0.3655038,0.3655038,0.3655038],[0.32819605,0.32819605,0.32819605],[0.29088253,0.29088253,0.29088253],[0.25356585,0.25356585,0.25356585],[0.21624726,0.21624726,0.21624726],[0.17892766,0.17892766,0.17892766],[0.1416074,0.1416074,0.1416074],[0.10428679,0.10428679,0.10428679]],[[0.06696367,0.06696367,0.06696367],[0.10428339,0.10428339,0.10428339],[0.14160246,0.14160246,0.14160246],[0.17892021,0.17892021,0.17892021],[0.21623588,0.21623588,0.21623588],[0.25354826,0.25354826,0.25354826],[0.29085463,0.29085463,0.29085463],[0.32815063,0.32815063,0.32815063],[0.3654279,0.3654279,0.3654279],[0.4026698,0.4026698,0.4026698],[0.43984264,0.43984264,0.43984264],[0.47687507,0.47687507,0.47687507],[0.51360947,0.51360947,0.51360947],[0.54968387,0.54968387,0.54968387],[0.5842546,0.5842546,0.5842546],[0.61545444,0.61545444,0.61545444],[0.64,0.64,0.64],[0.65496874,0.65496874,0.65496874],[0.6614814,0.6614814,0.6614814],[0.6635301,0.6635301,0.6635301],[0.6640142,0.6640142,0.6640142],[0.664098,0.664098,0.664098],[0.66410756,0.66410756,0.66410756],[0.6641081,0.6641081,0.6641081],[0.66410816,0.66410816,0.66410816],[0.66410816,0.66410816,0.66410816],[0.66410816,0.66410816,0.66410816],[0.6641081,0.6641081,0.6641081],[0.66410756,0.66410756,0.66410756],[0.664098,0.664098,0.664098],[0.6640142,0.6640142,0.6640142],[0.6635301,0.6635301,0.6635301],[0.6614814,0.6614814,0.6614814],[0.65496874,0.65496874,0.65496874],[0.64,0.64,0.64],[0.61545444,0.61545444,0.61545444],[0.5842546,0.5842546,0.5842546],[0.54968387,0.54968387,0.54968387],[0.51360947,0.51360947,0.51360947],[0.47687507,0.47687507,0.47687507],[0.43984264,0.43984264,0.43984264],[0.4026698,0.4026698,0.4026698],[0.3654279,0.3654279,0.3654279],[0.32815063,0.32815063,0.32815063],[0.29085463,0.29085463,0.29085463],[0.25354826,0.25354826,0.25354826],[0.21623588,0.21623588,0.21623588],[0.17892021,0.17892021,0.17892021],[0.14160246,0.14160246,0.14160246],[0.10428339,0.10428339,0.10428339]],[[0.066957235,0.066957235,0.066957235],[0.10427421,0.10427421,0.10427421],[0.14158893,0.14158893,0.14158893],[0.17890006,0.17890006,0.17890006],[0.21620536,0.21620536,0.21620536],[0.25350076,0.25350076,0.25350076],[0.29077935,0.29077935,0.29077935],[0.32802838,0.32802838,0.32802838],[0.36522353,0.36522353,0.36522353],[0.402318,0.402318,0.402318],[0.4392169,0.4392169,0.4392169],[0.47572267,0.47572267,0.47572267],[0.51141334,0.51141334,0.51141334],[0.5453886,0.5453886,0.5453886],[0.57585764,0.57585764,0.57585764],[0.6,0.6,0.6],[0.61545444,0.61545444,0.61545444],[0.62296087,0.62296087,0.62296087],[0.6257458,0.6257458,0.6257458],[0.62656176,0.62656176,0.62656176],[0.62675035,0.62675035,0.62675035],[0.6267829,0.6267829,0.6267829],[0.6267866,0.6267866,0.6267866],[0.6267868,0.6267868,0.6267868],[0.6267868,0.6267868,0.6267868],[0.6267868,0.6267868,0.6267868],[0.6267868,0.6267868,0.6267868],[0.6267868,0.6267868,0.6267868],[0.6267866,0.6267866,0.6267866],[0.6267829,0.6267829,0.6267829],[0.62675035,0.62675035,0.62675035],[0.62656176,0.62656176,0.62656176],[0.6257458,0.6257458,0.6257458],[0.62296087,0.62296087,0.62296087],[0.61545444,0.61545444,0.61545444],[0.6,0.6,0.6],[0.57585764,0.57585764,0.57585764],[0.5453886,0.5453886,0.5453886],[0.51141334,0.51141334,0.51141334],[0.47572267,0.47572267,0.47572267],[0.4392169,0.4392169,0.4392169],[0.402318,0.402318,0.402318],[0.36522353,0.36522353,0.36522353],[0.32802838,0.32802838,0.32802838],[0.29077935,0.29077935,0.29077935],[0.25350076,0.25350076,0.25350076],[0.21620536,0.21620536,0.21620536],[0.17890006,0.17890006,0.17890006],[0.14158893,0.14158893,0.14158893],[0.10427421,0.10427421,0.10427421]],[[0.06694168,0.06694168,0.06694168],[0.10425174,0.10425174,0.10425174],[0.14155596,0.14155596,0.14155596],[0.17885089,0.17885089,0.17885089],[0.21613055,0.21613055,0.21613055],[0.25338483,0.25338483,0.25338483],[0.29059547,0.29059547,0.29059547],[0.32772982,0.32772982,0.32772982],[0.36472595,0.36472595,0.36472595],[0.401465,0.401465,0.401465],[0.43771172,0.43771172,0.43771172],[0.47299528,0.47299528,0.47299528],[0.50638616,0.50638616,0.50638616],[0.53619707,0.53619707,0.53619707],[0.56,0.56,0.56],[0.57585764,0.57585764,0.57585764],[0.5842546,0.5842546,0.5842546],[0.5877968,0.5877968,0.5877968],[0.5890206,0.5890206,0.5890206],[0.5893699,0.5893699,0.5893699],[0.58945,0.58945,0.58945],[0.58946383,0.58946383,0.58946383],[0.5894654,0.5894654,0.5894654],[0.5894655,0.5894655,0.5894655],[0.5894655,0.5894655,0.5894655],[0.5894655,0.5894655,0.5894655],[0.5894655,0.5894655,0.5894655],[0.5894655,0.5894655,0.5894655],[0.5894654,0.5894654,0.5894654],[0.58946383,0.58946383,0.58946383],[0.58945,0.58945,0.58945],[0.5893699,0.5893699,0.5893699],[0.5890206,0.5890206,0.5890206],[0.5877968,0.5877968,0.5877968],[0.5842546,0.5842546,0.5842546],[0.57585764,0.57585764,0.57585764],[0.56,0.56,0.56],[0.53619707,0.53619707,0.53619707],[0.50638616,0.50638616,0.50638616],[0.47299528,0.47299528,0.47299528],[0.43771172,0.43771172,0.43771172],[0.401465,0.401465,0.401465],[0.36472595,0.36472595,0.36472595],[0.32772982,0.32772982,0.32772982],[0.29059547,0.29059547,0.29059547],[0.25338483,0.25338483,0.25338483],[0.21613055,0.21613055,0.21613055],[0.17885089,0.17885089,0.17885089],[0.14155596,0.14155596,0.14155596],[0.10425174,0.10425174,0.10425174]],[[0.06690651,0.06690651,0.06690651],[0.1042009,0.1042009,0.1042009],[0.1414814,0.1414814,0.1414814],[0.17873973,0.17873973,0.17873973],[0.21596181,0.21596181,0.21596181],[0.25312346,0.25312346,0.25312346],[0.29018205,0.29018205,0.29018205],[0.32706022,0.32706022,0.32706022],[0.3636154,0.3636154,0.3636154],[0.3995785,0.3995785,0.3995785],[0.43444133,0.43444133,0.43444133],[0.46726775,0.46726775,0.46726775],[0.4964863,0.4964863,0.4964863],[0.52,0.52,0.52],[0.53619707,0.53619707,0.53619707],[0.5453886,0.5453886,0.5453886],[0.54968387,0.54968387,0.54968387],[0.5513735,0.5513735,0.5513735],[0.5519403,0.5519403,0.5519403],[0.5521004,0.5521004,0.5521004],[0.55213714,0.55213714,0.55213714],[0.5521434,0.5521434,0.5521434],[0.5521441,0.5521441,0.5521441],[0.55214417,0.55214417,0.55214417],[0.55214417,0.55214417,0.55214417],[0.55214417,0.55214417,0.55214417],[0.55214417,0.55214417,0.55214417],[0.55214417,0.55214417,0.55214417],[0.5521441,0.5521441,0.5521441],[0.5521434,0.5521434,0.5521434],[0.55213714,0.55213714,0.55213714],[0.5521004,0.5521004,0.5521004],[0.5519403,0.5519403,0.5519403],[0.5513735,0.5513735,0.5513735],[0.54968387,0.54968387,0.54968387],[0.5453886,0.5453886,0.5453886],[0.53619707,0.53619707,0.53619707],[0.52,0.52,0.52],[0.4964863,0.4964863,0.4964863],[0.46726775,0.46726775,0.46726775],[0.43444133,0.43444133,0.43444133],[0.3995785,0.3995785,0.3995785],[0.3636154,0.3636154,0.3636154],[0.32706022,0.32706022,0.32706022],[0.29018205,0.29018205,0.29018205],[0.25312346,0.25312346,0.25312346],[0.21596181,0.21596181,0.21596181],[0.17873973,0.17873973,0.17873973],[0.1414814,0.1414814,0.1414814],[0.1042009,0.1042009,0.1042009]],[[0.066832244,0.066832244,0.066832244],[0.10409379,0.10409379,0.10409379],[0.14132446,0.14132446,0.14132446],[0.17850584,0.17850584,0.17850584],[0.21560693,0.21560693,0.21560693],[0.2525748,0.2525748,0.2525748],[0.28931636,0.28931636,0.28931636],[0.32566643,0.32566643,0.32566643],[0.36132693,0.36132693,0.36132693],[0.39576352,0.39576352,0.39576352],[0.4280504,0.4280504,0.4280504],[0.45673555,0.45673555,0.45673555],[0.48000002,0.48000002,0.48000002],[0.4964863,0.4964863,0.4964863],[0.50638616,0.50638616,0.50638616],[0.51141334,0.51141334,0.51141334],[0.51360947,0.51360947,0.51360947],[0.51444626,0.51444626,0.51444626],[0.51472354,0.51472354,0.51472354],[0.5148016,0.5148016,0.5148016],[0.5148194,0.5148194,0.5148194],[0.5148225,0.5148225,0.5148225],[0.51482284,0.51482284,0.51482284],[0.51482284,0.51482284,0.51482284],[0.51482284,0.51482284,0.51482284],[0.51482284,0.51482284,0.51482284],[0.51482284,0.51482284,0.51482284],[0.51482284,0.51482284,0.51482284],[0.51482284,0.51482284,0.51482284],[0.5148225,0.5148225,0.5148225],[0.5148194,0.5148194,0.5148194],[0.5148016,0.5148016,0.5148016],[0.51472354,0.51472354,0.51472354],[0.51444626,0.51444626,0.51444626],[0.51360947,0.51360947,0.51360947],[0.51141334,0.51141334,0.51141334],[0.50638616,0.50638616,0.50638616],[0.4964863,0.4964863,0.4964863],[0.48000002,0.48000002,0.48000002],[0.45673555,0.45673555,0.45673555],[0.4280504,0.4280504,0.4280504],[0.39576352,0.39576352,0.39576352],[0.36132693,0.36132693,0.36132693],[0.32566643,0.32566643,0.32566643],[0.28931636,0.28931636,0.28931636],[0.2525748,0.2525748,0.2525748],[0.21560693,0.21560693,0.21560693],[0.17850584,0.17850584,0.17850584],[0.14132446,0.14132446,0.14132446],[0.10409379,0.10409379,0.10409379]],[[0.066684365,0.066684365,0.066684365],[0.103880584,0.103880584,0.103880584],[0.14101219,0.14101219,0.14101219],[0.17804116,0.17804116,0.17804116],[0.21490365,0.21490365,0.21490365],[0.2514915,0.2514915,0.2514915],[0.28761846,0.28761846,0.28761846],[0.3229627,0.3229627,0.3229627],[0.35697365,0.35697365,0.35697365],[0.38874853,0.38874853,0.38874853],[0.41695243,0.41695243,0.41695243],[0.44,0.44,0.44],[0.45673555,0.45673555,0.45673555],[0.46726775,0.46726775,0.46726775],[0.47299528,0.47299528,0.47299528],[0.47572267,0.47572267,0.47572267],[0.47687507,0.47687507,0.47687507],[0.47730792,0.47730792,0.47730792],[0.4774505,0.4774505,0.4774505],[0.4774906,0.4774906,0.4774906],[0.47749978,0.47749978,0.47749978],[0.47750133,0.47750133,0.47750133],[0.4775015,0.4775015,0.4775015],[0.4775015,0.4775015,0.4775015],[0.4775015,0.4775015,0.4775015],[0.4775015,0.4775015,0.4775015],[0.4775015,0.4775015,0.4775015],[0.4775015,0.4775015,0.4775015],[0.4775015,0.4775015,0.4775015],[0.47750133,0.47750133,0.47750133],[0.47749978,0.47749978,0.47749978],[0.4774906,0.4774906,0.4774906],[0.4774505,0.4774505,0.4774505],[0.47730792,0.47730792,0.47730792],[0.47687507,0.47687507,0.47687507],[0.47572267,0.47572267,0.47572267],[0.47299528,0.47299528,0.47299528],[0.46726775,0.46726775,0.46726775],[0.45673555,0.45673555,0.45673555],[0.44,0.44,0.44],[0.41695243,0.41695243,0.41695243],[0.38874853,0.38874853,0.38874853],[0.35697365,0.35697365,0.35697365],[0.3229627,0.3229627,0.3229627],[0.28761846,0.28761846,0.28761846],[0.2514915,0.2514915,0.2514915],[0.21490365,0.21490365,0.21490365],[0.17804116,0.17804116,0.17804116],[0.14101219,0.14101219,0.14101219],[0.103880584,0.103880584,0.103880584]],[[0.06640434,0.06640434,0.06640434],[0.103477,0.103477,0.103477],[0.14042222,0.14042222,0.14042222],[0.17716551,0.17716551,0.17716551],[0.21358395,0.21358395,0.21358395],[0.24947315,0.24947315,0.24947315],[0.28449202,0.28449202,0.28449202],[0.31808287,0.31808287,0.31808287],[0.34937418,0.34937418,0.34937418],[0.37714267,0.37714267,0.37714267],[0.39999998,0.39999998,0.39999998],[0.41695243,0.41695243,0.41695243],[0.4280504,0.4280504,0.4280504],[0.43444133,0.43444133,0.43444133],[0.43771172,0.43771172,0.43771172],[0.4392169,0.4392169,0.4392169],[0.43984264,0.43984264,0.43984264],[0.44007605,0.44007605,0.44007605],[0.44015282,0.44015282,0.44015282],[0.44017434,0.44017434,0.44017434],[0.4401793,0.4401793,0.4401793],[0.44018012,0.44018012,0.44018012],[0.44018018,0.44018018,0.44018018],[0.44018018,0.44018018,0.44018018],[0.44018018,0.44018018,0.44018018],[0.44018018,0.44018018,0.44018018],[0.44018018,0.44018018,0.44018018],[0.44018018,0.44018018,0.44018018],[0.44018018,0.44018018,0.44018018],[0.44018012,0.44018012,0.44018012],[0.4401793,0.4401793,0.4401793],[0.44017434,0.44017434,0.44017434],[0.44015282,0.44015282,0.44015282],[0.44007605,0.44007605,0.44007605],[0.43984264,0.43984264,0.43984264],[0.4392169,0.4392169,0.4392169],[0.43771172,0.43771172,0.43771172],[0.43444133,0.43444133,0.43444133],[0.4280504,0.4280504,0.4280504],[0.41695243,0.41695243,0.41695243],[0.39999998,0.39999998,0.39999998],[0.37714267,0.37714267,0.37714267],[0.34937418,0.34937418,0.34937418],[0.31808287,0.31808287,0.31808287],[0.28449202,0.28449202,0.28449202],[0.24947315,0.24947315,0.24947315],[0.21358395,0.21358395,0.21358395],[0.17716551,0.17716551,0.17716551],[0.14042222,0.14042222,0.14042222],[0.103477,0.103477,0.103477]],[[0.06589687,0.06589687,0.06589687],[0.10274702,0.10274702,0.10274702],[0.1393581,0.1393581,0.1393581],[0.17559367,0.17559367,0.17559367],[0.21123308,0.21123308,0.21123308],[0.24592173,0.24592173,0.24592173],[0.2791012,0.2791012,0.2791012],[0.30993742,0.30993742,0.30993742],[0.3373109,0.3373109,0.3373109],[0.36,0.36,0.36],[0.37714267,0.37714267,0.37714267],[0.38874853,0.38874853,0.38874853],[0.39576352,0.39576352,0.39576352],[0.3995785,0.3995785,0.3995785],[0.401465,0.401465,0.401465],[0.402318,0.402318,0.402318],[0.4026698,0.4026698,0.4026698],[0.40280062,0.40280062,0.40280062],[0.40284353,0.40284353,0.40284353],[0.40285563,0.40285563,0.40285563],[0.40285838,0.40285838,0.40285838],[0.40285885,0.40285885,0.40285885],[0.4028589,0.4028589,0.4028589],[0.4028589,0.4028589,0.4028589],[0.4028589,0.4028589,0.4028589],[0.4028589,0.4028589,0.4028589],[0.4028589,0.4028589,0.4028589],[0.4028589,0.4028589,0.4028589],[0.4028589,0.4028589,0.4028589],[0.40285885,0.40285885,0.40285885],[0.40285838,0.40285838,0.40285838],[0.40285563,0.40285563,0.40285563],[0.40284353,0.40284353,0.40284353],[0.40280062,0.40280062,0.40280062],[0.4026698,0.4026698,0.4026698],[0.402318,0.402318,0.402318],[0.401465,0.401465,0.401465],[0.3995785,0.3995785,0.3995785],[0.39576352,0.39576352,0.39576352],[0.38874853,0.38874853,0.38874853],[0.37714267,0.37714267,0.37714267],[0.36,0.36,0.36],[0.3373109,0.3373109,0.3373109],[0.30993742,0.30993742,0.30993742],[0.2791012,0.2791012,0.2791012],[0.24592173,0.24592173,0.24592173],[0.21123308,0.21123308,0.21123308],[0.17559367,0.17559367,0.17559367],[0.1393581,0.1393581,0.1393581],[0.10274702,0.10274702,0.10274702]],[[0.06501317,0.06501317,0.06501317],[0.101480305,0.101480305,0.101480305],[0.13752115,0.13752115,0.13752115],[0.17290193,0.17290193,0.17290193],[0.20725858,0.20725858,0.20725858],[0.2400381,0.2400381,0.2400381],[0.27044666,0.27044666,0.27044666],[0.29746062,0.29746062,0.29746062],[0.32,0.32,0.32],[0.3373109,0.3373109,0.3373109],[0.34937418,0.34937418,0.34937418],[0.35697365,0.35697365,0.35697365],[0.36132693,0.36132693,0.36132693],[0.3636154,0.3636154,0.3636154],[0.36472595,0.36472595,0.36472595],[0.36522353,0.36522353,0.36522353],[0.3654279,0.3654279,0.3654279],[0.3655038,0.3655038,0.3655038],[0.3655287,0.3655287,0.3655287],[0.36553568,0.36553568,0.36553568],[0.36553723,0.36553723,0.36553723],[0.36553752,0.36553752,0.36553752],[0.36553758,0.36553758,0.36553758],[0.36553758,0.36553758,0.36553758],[0.36553758,0.36553758,0.36553758],[0.36553758,0.36553758,0.36553758],[0.36553758,0.36553758,0.36553758],[0.36553758,0.36553758,0.36553758],[0.36553758,0.36553758,0.36553758],[0.36553752,0.36553752,0.36553752],[0.36553723,0.36553723,0.36553723],[0.36553568,0.36553568,0.36553568],[0.3655287,0.3655287,0.3655287],[0.3655038,0.3655038,0.3655038],[0.3654279,0.3654279,0.3654279],[0.36522353,0.36522353,0.36522353],[0.36472595,0.36472595,0.36472595],[0.3636154,0.3636154,0.3636154],[0.36132693,0.36132693,0.36132693],[0.35697365,0.35697365,0.35697365],[0.34937418,0.34937418,0.34937418],[0.3373109,0.3373109,0.3373109],[0.32,0.32,0.32],[0.29746062,0.29746062,0.29746062],[0.27044666,0.27044666,0.27044666],[0.2400381,0.2400381,0.2400381],[0.20725858,0.20725858,0.20725858],[0.17290193,0.17290193,0.17290193],[0.13752115,0.13752115,0.13752115],[0.101480305,0.101480305,0.101480305]],[[0.06353134,0.06353134,0.06353134],[0.09936774,0.09936774,0.09936774],[0.13448352,0.13448352,0.13448352],[0.16850913,0.16850913,0.16850913],[0.20090163,0.20090163,0.20090163],[0.23090887,0.23090887,0.23090887],[0.25759476,0.25759476,0.25759476],[0.27999997,0.27999997,0.27999997],[0.29746062,0.29746062,0.29746062],[0.30993742,0.30993742,0.30993742],[0.31808287,0.31808287,0.31808287],[0.3229627,0.3229627,0.3229627],[0.32566643,0.32566643,0.32566643],[0.32706022,0.32706022,0.32706022],[0.32772982,0.32772982,0.32772982],[0.32802838,0.32802838,0.32802838],[0.32815063,0.32815063,0.32815063],[0.32819605,0.32819605,0.32819605],[0.32821095,0.32821095,0.32821095],[0.32821512,0.32821512,0.32821512],[0.32821608,0.32821608,0.32821608],[0.3282162,0.3282162,0.3282162],[0.3282163,0.3282163,0.3282163],[0.3282163,0.3282163,0.3282163],[0.3282163,0.3282163,0.3282163],[0.3282163,0.3282163,0.3282163],[0.3282163,0.3282163,0.3282163],[0.3282163,0.3282163,0.3282163],[0.3282163,0.3282163,0.3282163],[0.3282162,0.3282162,0.3282162],[0.32821608,0.32821608,0.32821608],[0.32821512,0.32821512,0.32821512],[0.32821095,0.32821095,0.32821095],[0.32819605,0.32819605,0.32819605],[0.32815063,0.32815063,0.32815063],[0.32802838,0.32802838,0.32802838],[0.32772982,0.32772982,0.32772982],[0.32706022,0.32706022,0.32706022],[0.32566643,0.32566643,0.32566643],[0.3229627,0.3229627,0.3229627],[0.31808287,0.31808287,0.31808287],[0.30993742,0.30993742,0.30993742],[0.29746062,0.29746062,0.29746062],[0.27999997,0.27999997,0.27999997],[0.25759476,0.25759476,0.25759476],[0.23090887,0.23090887,0.23090887],[0.20090163,0.20090163,0.20090163],[0.16850913,0.16850913,0.16850913],[0.13448352,0.13448352,0.13448352],[0.09936774,0.09936774,0.09936774]],[[0.06113547,0.06113547,0.06113547],[0.095982194,0.095982194,0.095982194],[0.12967998,0.12967998,0.12967998],[0.16169941,0.16169941,0.16169941],[0.19133008,0.19133008,0.19133008],[0.21771562,0.21771562,0.21771562],[0.24000001,0.24000001,0.24000001],[0.25759476,0.25759476,0.25759476],[0.27044666,0.27044666,0.27044666],[0.2791012,0.2791012,0.2791012],[0.28449202,0.28449202,0.28449202],[0.28761846,0.28761846,0.28761846],[0.28931636,0.28931636,0.28931636],[0.29018205,0.29018205,0.29018205],[0.29059547,0.29059547,0.29059547],[0.29077935,0.29077935,0.29077935],[0.29085463,0.29085463,0.29085463],[0.29088253,0.29088253,0.29088253],[0.2908917,0.2908917,0.2908917],[0.29089427,0.29089427,0.29089427],[0.2908948,0.2908948,0.2908948],[0.29089493,0.29089493,0.29089493],[0.290895,0.290895,0.290895],[0.290895,0.290895,0.290895],[0.290895,0.290895,0.290895],[0.290895,0.290895,0.290895],[0.290895,0.290895,0.290895],[0.290895,0.290895,0.290895],[0.290895,0.290895,0.290895],[0.29089493,0.29089493,0.29089493],[0.2908948,0.2908948,0.2908948],[0.29089427,0.29089427,0.29089427],[0.2908917,0.2908917,0.2908917],[0.29088253,0.29088253,0.29088253],[0.29085463,0.29085463,0.29085463],[0.29077935,0.29077935,0.29077935],[0.29059547,0.29059547,0.29059547],[0.29018205,0.29018205,0.29018205],[0.28931636,0.28931636,0.28931636],[0.28761846,0.28761846,0.28761846],[0.28449202,0.28449202,0.28449202],[0.2791012,0.2791012,0.2791012],[0.27044666,0.27044666,0.27044666],[0.25759476,0.25759476,0.25759476],[0.24000001,0.24000001,0.24000001],[0.21771562,0.21771562,0.21771562],[0.19133008,0.19133008,0.19133008],[0.16169941,0.16169941,0.16169941],[0.12967998,0.12967998,0.12967998],[0.095982194,0.095982194,0.095982194]],[[0.057402194,0.057402194,0.057402194],[0.0907771,0.0907771,0.0907771],[0.122437775,0.122437775,0.122437775],[0.15171522,0.15171522,0.15171522],[0.17782497,0.17782497,0.17782497],[0.19999999,0.19999999,0.19999999],[0.21771562,0.21771562,0.21771562],[0.23090887,0.23090887,0.23090887],[0.2400381,0.2400381,0.2400381],[0.24592173,0.24592173,0.24592173],[0.24947315,0.24947315,0.24947315],[0.2514915,0.2514915,0.2514915],[0.2525748,0.2525748,0.2525748],[0.25312346,0.25312346,0.25312346],[0.25338483,0.25338483,0.25338483],[0.25350076,0.25350076,0.25350076],[0.25354826,0.25354826,0.25354826],[0.25356585,0.25356585,0.25356585],[0.25357157,0.25357157,0.25357157],[0.25357318,0.25357318,0.25357318],[0.2535736,0.2535736,0.2535736],[0.2535736,0.2535736,0.2535736],[0.25357366,0.25357366,0.25357366],[0.25357366,0.25357366,0.25357366],[0.25357366,0.25357366,0.25357366],[0.25357366,0.25357366,0.25357366],[0.25357366,0.25357366,0.25357366],[0.25357366,0.25357366,0.25357366],[0.25357366,0.25357366,0.25357366],[0.2535736,0.2535736,0.2535736],[0.2535736,0.2535736,0.2535736],[0.25357318,0.25357318,0.25357318],[0.25357157,0.25357157,0.25357157],[0.25356585,0.25356585,0.25356585],[0.25354826,0.25354826,0.25354826],[0.25350076,0.25350076,0.25350076],[0.25338483,0.25338483,0.25338483],[0.25312346,0.25312346,0.25312346],[0.2525748,0.2525748,0.2525748],[0.2514915,0.2514915,0.2514915],[0.24947315,0.24947315,0.24947315],[0.24592173,0.24592173,0.24592173],[0.2400381,0.2400381,0.2400381],[0.23090887,0.23090887,0.23090887],[0.21771562,0.21771562,0.21771562],[0.19999999,0.19999999,0.19999999],[0.17782497,0.17782497,0.17782497],[0.15171522,0.15171522,0.15171522],[0.122437775,0.122437775,0.122437775],[0.0907771,0.0907771,0.0907771]],[[0.051806152,0.051806152,0.051806152],[0.08312279,0.08312279,0.08312279],[0.11206865,0.11206865,0.11206865],[0.13792437,0.13792437,0.13792437],[0.16000003,0.16000003,0.16000003],[0.17782497,0.17782497,0.17782497],[0.19133008,0.19133008,0.19133008],[0.20090163,0.20090163,0.20090163],[0.20725858,0.20725858,0.20725858],[0.21123308,0.21123308,0.21123308],[0.21358395,0.21358395,0.21358395],[0.21490365,0.21490365,0.21490365],[0.21560693,0.21560693,0.21560693],[0.21596181,0.21596181,0.21596181],[0.21613055,0.21613055,0.21613055],[0.21620536,0.21620536,0.21620536],[0.21623588,0.21623588,0.21623588],[0.21624726,0.21624726,0.21624726],[0.21625096,0.21625096,0.21625096],[0.21625197,0.21625197,0.21625197],[0.21625227,0.21625227,0.21625227],[0.21625227,0.21625227,0.21625227],[0.21625227,0.21625227,0.21625227],[0.21625227,0.21625227,0.21625227],[0.21625227,0.21625227,0.21625227],[0.21625227,0.21625227,0.21625227],[0.21625227,0.21625227,0.21625227],[0.21625227,0.21625227,0.21625227],[0.21625227,0.21625227,0.21625227],[0.21625227,0.21625227,0.21625227],[0.21625227,0.21625227,0.21625227],[0.21625197,0.21625197,0.21625197],[0.21625096,0.21625096,0.21625096],[0.21624726,0.21624726,0.21624726],[0.21623588,0.21623588,0.21623588],[0.21620536,0.21620536,0.21620536],[0.21613055,0.21613055,0.21613055],[0.21596181,0.21596181,0.21596181],[0.21560693,0.21560693,0.21560693],[0.21490365,0.21490365,0.21490365],[0.21358395,0.21358395,0.21358395],[0.21123308,0.21123308,0.21123308],[0.20725858,0.20725858,0.20725858],[0.20090163,0.20090163,0.20090163],[0.19133008,0.19133008,0.19133008],[0.17782497,0.17782497,0.17782497],[0.16000003,0.16000003,0.16000003],[0.13792437,0.13792437,0.13792437],[0.11206865,0.11206865,0.11206865],[0.08312279,0.08312279,0.08312279]],[[0.043759525,0.043759525,0.043759525],[0.07239407,0.07239407,0.07239407],[0.09801525,0.09801525,0.09801525],[0.120000005,0.120000005,0.120000005],[0.13792437,0.13792437,0.13792437],[0.15171522,0.15171522,0.15171522],[0.16169941,0.16169941,0.16169941],[0.16850913,0.16850913,0.16850913],[0.17290193,0.17290193,0.17290193],[0.17559367,0.17559367,0.17559367],[0.17716551,0.17716551,0.17716551],[0.17804116,0.17804116,0.17804116],[0.17850584,0.17850584,0.17850584],[0.17873973,0.17873973,0.17873973],[0.17885089,0.17885089,0.17885089],[0.17890006,0.17890006,0.17890006],[0.17892021,0.17892021,0.17892021],[0.17892766,0.17892766,0.17892766],[0.1789301,0.1789301,0.1789301],[0.17893082,0.17893082,0.17893082],[0.17893094,0.17893094,0.17893094],[0.17893094,0.17893094,0.17893094],[0.17893094,0.17893094,0.17893094],[0.17893094,0.17893094,0.17893094],[0.17893094,0.17893094,0.17893094],[0.17893094,0.17893094,0.17893094],[0.17893094,0.17893094,0.17893094],[0.17893094,0.17893094,0.17893094],[0.17893094,0.17893094,0.17893094],[0.17893094,0.17893094,0.17893094],[0.17893094,0.17893094,0.17893094],[0.17893082,0.17893082,0.17893082],[0.1789301,0.1789301,0.1789301],[0.17892766,0.17892766,0.17892766],[0.17892021,0.17892021,0.17892021],[0.17890006,0.17890006,0.17890006],[0.17885089,0.17885089,0.17885089],[0.17873973,0.17873973,0.17873973],[0.17850584,0.17850584,0.17850584],[0.17804116,0.17804116,0.17804116],[0.17716551,0.17716551,0.17716551],[0.17559367,0.17559367,0.17559367],[0.17290193,0.17290193,0.17290193],[0.16850913,0.16850913,0.16850913],[0.16169941,0.16169941,0.16169941],[0.15171522,0.15171522,0.15171522],[0.13792437,0.13792437,0.13792437],[0.120000005,0.120000005,0.120000005],[0.09801525,0.09801525,0.09801525],[0.07239407,0.07239407,0.07239407]],[[0.03269446,0.03269446,0.03269446],[0.058098435,0.058098435,0.058098435],[0.07999998,0.07999998,0.07999998],[0.09801525,0.09801525,0.09801525],[0.11206865,0.11206865,0.11206865],[0.122437775,0.122437775,0.122437775],[0.12967998,0.12967998,0.12967998],[0.13448352,0.13448352,0.13448352],[0.13752115,0.13752115,0.13752115],[0.1393581,0.1393581,0.1393581],[0.14042222,0.14042222,0.14042222],[0.14101219,0.14101219,0.14101219],[0.14132446,0.14132446,0.14132446],[0.1414814,0.1414814,0.1414814],[0.14155596,0.14155596,0.14155596],[0.14158893,0.14158893,0.14158893],[0.14160246,0.14160246,0.14160246],[0.1416074,0.1416074,0.1416074],[0.14160907,0.14160907,0.14160907],[0.14160955,0.14160955,0.14160955],[0.14160961,0.14160961,0.14160961],[0.14160961,0.14160961,0.14160961],[0.14160961,0.14160961,0.14160961],[0.14160961,0.14160961,0.14160961],[0.14160961,0.14160961,0.14160961],[0.14160961,0.14160961,0.14160961],[0.14160961,0.14160961,0.14160961],[0.14160961,0.14160961,0.14160961],[0.14160961,0.14160961,0.14160961],[0.14160961,0.14160961,0.14160961],[0.14160961,0.14160961,0.14160961],[0.14160955,0.14160955,0.14160955],[0.14160907,0.14160907,0.14160907],[0.1416074,0.1416074,0.1416074],[0.14160246,0.14160246,0.14160246],[0.14158893,0.14158893,0.14158893],[0.14155596,0.14155596,0.14155596],[0.1414814,0.1414814,0.1414814],[0.14132446,0.14132446,0.14132446],[0.14101219,0.14101219,0.14101219],[0.14042222,0.14042222,0.14042222],[0.1393581,0.1393581,0.1393581],[0.13752115,0.13752115,0.13752115],[0.13448352,0.13448352,0.13448352],[0.12967998,0.12967998,0.12967998],[0.122437775,0.122437775,0.122437775],[0.11206865,0.11206865,0.11206865],[0.09801525,0.09801525,0.09801525],[0.07999998,0.07999998,0.07999998],[0.058098435,0.058098435,0.058098435]],[[0.018175125,0.018175125,0.018175125],[0.04000002,0.04000002,0.04000002],[0.058098435,0.058098435,0.058098435],[0.07239407,0.07239407,0.07239407],[0.08312279,0.08312279,0.08312279],[0.0907771,0.0907771,0.0907771],[0.095982194,0.095982194,0.095982194],[0.09936774,0.09936774,0.09936774],[0.101480305,0.101480305,0.101480305],[0.10274702,0.10274702,0.10274702],[0.103477,0.103477,0.103477],[0.103880584,0.103880584,0.103880584],[0.10409379,0.10409379,0.10409379],[0.1042009,0.1042009,0.1042009],[0.10425174,0.10425174,0.10425174],[0.10427421,0.10427421,0.10427421],[0.10428339,0.10428339,0.10428339],[0.10428679,0.10428679,0.10428679],[0.10428798,0.10428798,0.10428798],[0.10428822,0.10428822,0.10428822],[0.10428834,0.10428834,0.10428834],[0.10428834,0.10428834,0.10428834],[0.10428834,0.10428834,0.10428834],[0.10428834,0.10428834,0.10428834],[0.10428834,0.10428834,0.10428834],[0.10428834,0.10428834,0.10428834],[0.10428834,0.10428834,0.10428834],[0.10428834,0.10428834,0.10428834],[0.10428834,0.10428834,0.10428834],[0.10428834,0.10428834,0.10428834],[0.10428834,0.10428834,0.10428834],[0.10428822,0.10428822,0.10428822],[0.10428798,0.10428798,0.10428798],[0.10428679,0.10428679,0.10428679],[0.10428339,0.10428339,0.10428339],[0.10427421,0.10427421,0.10427421],[0.10425174,0.10425174,0.10425174],[0.1042009,0.1042009,0.1042009],[0.10409379,0.10409379,0.10409379],[0.103880584,0.103880584,0.103880584],[0.103477,0.103477,0.103477],[0.10274702,0.10274702,0.10274702],[0.101480305,0.101480305,0.101480305],[0.09936774,0.09936774,0.09936774],[0.095982194,0.095982194,0.095982194],[0.0907771,0.0907771,0.0907771],[0.08312285,0.08312285,0.08312285],[0.07239413,0.07239413,0.07239413],[0.058098495,0.058098495,0.058098495],[0.04000008,0.04000008,0.04000008]]],\"zmax\":[1,1,1,1],\"zmin\":[0,0,0,0],\"type\":\"image\",\"xaxis\":\"x\",\"yaxis\":\"y\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0]}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('e185c29b-7edb-4536-996c-9abbdae3ee65');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# ✏️ your code here\n",
        "p=10\n",
        "\n",
        "t = torch.arange(50)\n",
        "s = torch.arange(50).unsqueeze(1)\n",
        "\n",
        "res = (abs(t-a)**p + abs(s-b)**p)**(1/p)\n",
        "\n",
        "#normalize between 0 and 1\n",
        "res=(res-res.min())/res.max()\n",
        "res=1-res\n",
        "\n",
        "\n",
        "plot_row_images(res[None, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUDYSUxLuoAK"
      },
      "outputs": [],
      "source": [
        "# @title 👀 Solution\n",
        "\n",
        "\n",
        "rows = torch.arange(x.shape[0])\n",
        "cols = torch.arange(x.shape[1])\n",
        "\n",
        "# Manual computation of L1\n",
        "y = (torch.abs(rows - a)[:, None] + torch.abs(cols - b)[None, :])\n",
        "px.imshow(y).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYF38G6dwAbT"
      },
      "outputs": [],
      "source": [
        "# @title 👀 Solution\n",
        "\n",
        "\n",
        "# Parametric computation of Lp\n",
        "p = 8\n",
        "y = ((torch.abs(rows - a ) ** p )[:, None] +\n",
        "     (torch.abs(cols - b) ** p)[None, :]) ** (1/p)\n",
        "px.imshow(y).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O530uju9a0h"
      },
      "source": [
        "Try Solution 2 with `p=10`. What happens, and why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NYZfn5gJ5kOM"
      },
      "outputs": [],
      "source": [
        "# @title 👀 Solution\n",
        "\n",
        "\n",
        "# This works even with p=10. Why?\n",
        "p = 10\n",
        "y = ((torch.abs(rows.double() - a ) ** p )[:, None] +\n",
        "     (torch.abs(cols.double() - b) ** p)[None, :]) ** (1/p)\n",
        "px.imshow(y).show()\n",
        "\n",
        "# -> Write your own explanation here\n",
        "p = 10\n",
        "print(torch.tensor(10, dtype=torch.int) ** p)\n",
        "print(torch.tensor(10, dtype=torch.double) ** p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpE0yZGF_ytT"
      },
      "source": [
        "#### **Non-elementwise operations**\n",
        "\n",
        "\n",
        "PyTorch and NumPy provide many useful functions to perform computations on tensors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EI33i1Df_ytN"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([[1, 2, 3], [3, 4, 5]], dtype=torch.float32)\n",
        "print_arr(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6x4rhtfI_ytI"
      },
      "outputs": [],
      "source": [
        "# Sum up all the elements\n",
        "print_arr(torch.sum(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvndASPe_ytD"
      },
      "outputs": [],
      "source": [
        "# Compute the mean of each column\n",
        "print_arr(torch.mean(x, dim=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUwZxvZP_ysy"
      },
      "source": [
        "> **REMEMBER!**\n",
        ">\n",
        "> In order to avoid confusion with the `dim` parameter, you can think of it as an **index over the list returned by `tensor.shape`**. The operation is performed by iterating over that dimension.\n",
        ">\n",
        "> Example above: since our tensor `x` has shape `[2, 3]`, the dimension `dim=0` operates along the `2`.\n",
        ">\n",
        "> Visually (here array means _tensor_):\n",
        ">\n",
        "><img src=\"https://qph.fs.quoracdn.net/main-qimg-30be20ab9458b5865b526d287b4fef9a\" width=\"500\" >"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "puZDDaDfgDK8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4adf70f7-af8a-4ec9-8ed2-ddf9068d8696"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9067, 0.3375, 0.6883, 0.8696, 0.0681],\n",
            "        [0.4655, 0.2274, 0.8276, 0.1487, 0.4578],\n",
            "        [0.6013, 0.1243, 0.1309, 0.7365, 0.0483]]) <shape: torch.Size([3, 5])> <dtype: torch.float32>\n"
          ]
        }
      ],
      "source": [
        "print_arr(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "4K-4z5pL_ys-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48a9029c-1597-4d6d-ae99-843e1cd5c31c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0125, 0.0060, 0.0003]) <shape: torch.Size([3])> <dtype: torch.float32>\n"
          ]
        }
      ],
      "source": [
        "# Compute the product of each row\n",
        "print_arr(torch.prod(x, dim=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "MRtgzF33_ys4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1923e66-ae05-4193-be85-d4e1edc75285"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.9067, 0.3375, 0.8276, 0.8696, 0.4578]) <shape: torch.Size([5])> <dtype: torch.float32>\n"
          ]
        }
      ],
      "source": [
        "# Max along the rows (i.e. max value in each column)\n",
        "values, indices = torch.max(x, dim=0)\n",
        "print_arr(values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Hd0zbRp0_ys0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcb08481-dd8d-4e3a-b085-952864aec267"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.9067, 0.8276, 0.7365]) <shape: torch.Size([3])> <dtype: torch.float32>\n"
          ]
        }
      ],
      "source": [
        "# Max along the columns (i.e. max value in each row)\n",
        "values, indices = torch.max(x, dim=1)\n",
        "print_arr(values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1xEs8jkkk4f"
      },
      "source": [
        "##### 📖 **Dim parameter, let's take a peek under the hood**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwORY20xmzq8"
      },
      "source": [
        "Let's see what the `dim` parameter exactly does:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "DoDvtWHq_ysu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3628c065-b248-438a-983a-381452b169b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0,  1,  2,  3],\n",
            "         [ 4,  5,  6,  7],\n",
            "         [ 8,  9, 10, 11]],\n",
            "\n",
            "        [[12, 13, 14, 15],\n",
            "         [16, 17, 18, 19],\n",
            "         [20, 21, 22, 23]]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 6, 22, 38],\n",
              "        [54, 70, 86]])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "dim = 2\n",
        "\n",
        "a = torch.arange(2*3*4).reshape(2, 3, 4)\n",
        "print(a)\n",
        "out = a.sum(dim=dim)\n",
        "out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "9-KbxoTK_ysq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3684ab01-d7c9-4375-b95c-10a6e9cdd0b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# It is summing over the `dim` dimension, i.e.:\n",
        "a.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "mZcG-q5R_ysm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5283a082-a31d-487a-e8b1-68a62dca7270"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "# The `dim` dimension has 4 elements\n",
        "a.shape[dim]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "zqT4jSkW_ysi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31a92520-f9f0-4d1a-eb28-6db1ae8aa356"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "# The dimension dim collapses, the output tensor will have shape:\n",
        "new_shape = a.shape[:dim] + a.shape[dim + 1:]\n",
        "new_shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "GHakpxbl_ysd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e17a6d6c-2ccc-4c4b-ab0c-7f356bb12b68"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 6., 22., 38.],\n",
              "        [54., 70., 86.]])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "# Explicitly compute the sum over dim\n",
        "out = torch.zeros(new_shape)\n",
        "\n",
        "# iterate over all the rows\n",
        "for r in range(a.shape[0]):\n",
        "  # iterate over all the columns in the r-th row\n",
        "  for c in range(a.shape[1]):\n",
        "\n",
        "    for i in range(a.shape[dim]): # <- sum over 'dim'\n",
        "\n",
        "      out[r, c] += a[r, c, i]\n",
        "\n",
        "out\n",
        "\n",
        "# **DO NOT** use for loops in your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQA3ngqoHsEt"
      },
      "source": [
        "###### **EXERCISE**\n",
        ">\n",
        "> Given a matrix $X \\in R^{k \\times k}$:\n",
        "> - Compute the mean of the values along its diagonal.\n",
        ">\n",
        "> Perform this computation in at least two different ways, then check that the results are the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "evQYg9-GH-Td",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18fe5382-a134-4d8d-8c45-dfacdc3b6742"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.6926, 0.6158, 0.3863, 0.3743],\n",
            "        [0.7354, 0.5428, 0.2464, 0.2744],\n",
            "        [0.4057, 0.5559, 0.3150, 0.1416],\n",
            "        [0.3769, 0.2074, 0.7471, 0.9266]]) <shape: torch.Size([4, 4])> <dtype: torch.float32>\n"
          ]
        }
      ],
      "source": [
        "x = torch.rand(4, 4)\n",
        "print_arr(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "3x8-6wyGcB_R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af6099a0-71c6-4783-fc6d-6d7ee3525471"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean: tensor(0.6192)\n",
            "tensor(0.6192)\n",
            "tensor(0.6192)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.6192)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "# ✏️ your code here\n",
        "t = x * torch.eye(x.shape[0],x.shape[1])\n",
        "print('mean:', t.sum()/x.shape[0])\n",
        "print(torch.trace(x)/x.shape[0])\n",
        "print(torch.mean(x[torch.arange(x.shape[0]), torch.arange(x.shape[1])]))\n",
        "torch.mean(torch.diag(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fOEhv8X0ILC2"
      },
      "outputs": [],
      "source": [
        "# @title 👀 Solution\n",
        "\n",
        "\n",
        "a = torch.mean(x[torch.arange(x.shape[0]), torch.arange(x.shape[1])])\n",
        "b = torch.sum(torch.eye(x.shape[0]) * x) / x.shape[0]\n",
        "c = torch.trace(x) / x.shape[0]\n",
        "d = torch.mean(torch.diag(x))\n",
        "\n",
        "print(torch.equal(a, b) and torch.equal(a, c) and torch.equal(a, d))\n",
        "print_arr(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YePwf4ok_ysb"
      },
      "source": [
        "##### **EXERCISE**\n",
        ">\n",
        "> Given a binary non-symmetric matrix $X \\in \\{0, 1\\}^{n\\times n}$, build the symmetric matrix $Y \\in \\{0, 1\\}^{n \\times n}$ defined as:\n",
        "> $$\n",
        "y_{ij} =\n",
        "\\begin{cases}\n",
        "1 & \\text{if } x_{ij} = 1 \\\\\n",
        "1 & \\text{if } x_{ji} = 1 \\\\\n",
        "0 & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        ">\n",
        "> *Hint*: search for `clamp` in the [docs](https://pytorch.org/docs/stable/index.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "2Dv-OmnV_ysU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39c6a11f-0765-4ab9-8fa4-cfe056274a73"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 0, 1, 0],\n",
              "        [0, 1, 0, 0, 1],\n",
              "        [0, 0, 0, 1, 1],\n",
              "        [1, 1, 0, 0, 1],\n",
              "        [0, 1, 1, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "x = torch.randint(0, 2, (5, 5))  # Non-symmetric matrix\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "j6HT6OSElDic"
      },
      "outputs": [],
      "source": [
        "# ✏️ your code here\n",
        "a = (x+x.T).bool().int()\n",
        "b = (x + x.transpose(1, 0)).clamp(max=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tM6Yd14JrAB"
      },
      "outputs": [],
      "source": [
        "# @title 👀 Solution\n",
        "\n",
        "\n",
        "(x + x.transpose(1, 0)).clamp(max=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVC3YH8H_ysT"
      },
      "source": [
        "#### **Tensor contractions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QewYLJq-_yr5"
      },
      "source": [
        "##### **Matrix multiplication**\n",
        "\n",
        "Given $X \\in R^{n \\times d}$ and $Y \\in R^{d \\times v}$, their matrix multiplication $Z \\in R^{n \\times v}$ is defined as:\n",
        "\n",
        "$$ \\sum_{k=0}^{d} x_{ik} y_{kj} = z_{ij} $$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qu16frkd_yru"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
        "y = torch.tensor([[1, 2], [2, 1]])\n",
        "print_arr(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ag_EbhhOH1Xp"
      },
      "outputs": [],
      "source": [
        "# as we will see, matmul's functionality is not limited to matrix-matrix multiplication\n",
        "torch.matmul(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbVbXIzz_yrl"
      },
      "outputs": [],
      "source": [
        "x @ y  # Operator overload for matmul"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FKQsMIG_yrh"
      },
      "outputs": [],
      "source": [
        "torch.mm(x, y)  # PyTorch function, only works for rank-2 tensors (matrices) https://pytorch.org/docs/stable/generated/torch.mm.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfmI4xltpJlO"
      },
      "outputs": [],
      "source": [
        "x.mm(y)  # Tensor method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTqIXFNU_yrZ"
      },
      "outputs": [],
      "source": [
        "torch.einsum('ik, kj -> ij', (x, y))  # Einsum notation!\n",
        "\n",
        "# It summed up dimension labeled with the index `k`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-KPmcQg_ysS"
      },
      "source": [
        "##### **Dot product**\n",
        "Also known as scalar product or inner product.\n",
        "Given $x \\in \\mathbb{R}^k$ and $y \\in \\mathbb{R}^k$, the dot product $z \\in \\mathbb{R}$ is defined as:\n",
        "\n",
        "$$ \\sum_{i=0}^{k} x_i y_i = z $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DxmdUoC_ysM"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([1, 2, 3])\n",
        "y = torch.tensor([4, 5, 6])\n",
        "print_arr(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixf_iXF3KbOv"
      },
      "outputs": [],
      "source": [
        "# We want to perform:\n",
        "(1 * 4) + (2 * 5) + (3 * 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0jRMizc_ysG"
      },
      "outputs": [],
      "source": [
        "torch.dot(x, y)  # PyTorch function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xzISunbpeUi"
      },
      "outputs": [],
      "source": [
        "x.dot(y) # Tensor method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrW2utM9_ysA"
      },
      "outputs": [],
      "source": [
        "x @ y  # PyTorch operator again overloading matmul"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWV3hiAU_yr7"
      },
      "outputs": [],
      "source": [
        "torch.einsum('i, i ->', (x, y))  # Einstein notation!\n",
        "\n",
        "# Read it as:\n",
        "# - iterate with i along x\n",
        "# - iterate with i along y\n",
        "# - compute the product at each iteration\n",
        "# - sum the products and return a scalar (-> means return a scalar)\n",
        "\n",
        "# More in general, Einstein notation:\n",
        "# Multiply point-wise repeated indices in the input\n",
        "# Sum up along the indices that `do not` appear in the output\n",
        "\n",
        "# More on this below!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a9Yl0IC_yrW"
      },
      "source": [
        "##### **Batch matrix multiplication**\n",
        "\n",
        "Often we want to perform more operations together. Why?\n",
        "- Reduce the **overhead of uploading** each tensor to/from the GPU memory\n",
        "- **Better parallelization** of the computation\n",
        "\n",
        "Given two 3D tensors, each one containing ``b`` matrices,\n",
        "$X \\in \\mathbb{R}^{b \\times n \\times m}$\n",
        "and  \n",
        "$Y \\in \\mathbb{R}^{b \\times m \\times p}$,\n",
        "\n",
        "We want to multiply together each $i$-th pair of matrices, obtaining a tensor $Z \\in \\mathbb{R}^{b \\times n \\times p}$ defined as:\n",
        "\n",
        "$$ z_{bij} = \\sum_{k=0}^m x_{bik} y_{bkj} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwUMo2Br_yrQ"
      },
      "outputs": [],
      "source": [
        "# here b = 2 matrices\n",
        "x = torch.tensor([[[1, 2], [3, 4], [5, 6]], [[1, 2], [3, 4], [5, 6]]])  # 3x2 matrices\n",
        "y = torch.tensor([[[1, 2], [2, 1]], [[1, 2], [2, 1]]])  # 2x2 matrices\n",
        "print_arr(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDW_9XKJ_yrG"
      },
      "outputs": [],
      "source": [
        "torch.bmm(x, y)  # **not** torch.mm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zf7GaNmw_yrM"
      },
      "outputs": [],
      "source": [
        "# Operator overload! again, matmul is actually doing the job\n",
        "x @ y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjjRPx-n_yrC"
      },
      "outputs": [],
      "source": [
        "torch.einsum('bik, bkj -> bij', (x, y)) # Einstein notation!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-aYQyHz_yrA"
      },
      "source": [
        "##### 📖 **Broadcast matrix multiplication**\n",
        "\n",
        "Given a matrix $Y \\in \\mathbb{R}^{m \\times p}$ and $b$ matrices of size $n \\times m$ organized in a 3D tensor $X \\in \\mathbb{R}^{b \\times n \\times m}$, we want to multiply together each matrix $X_{i,:,:}$ with $Y$, obtaining a tensor $Z \\in R^{b \\times n \\times p}$ defined as:\n",
        "\n",
        "$$ z_{bij} = \\sum_{k=0}^m x_{bik} y_{kj} $$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awE5anPp_yq7"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([[[1, 2], [3, 4], [5, 6]], [[1, 2], [3, 4], [5, 6]]])\n",
        "y = torch.tensor([[1, 2], [2, 1]])\n",
        "print_arr(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvZyCiYN_yqz"
      },
      "outputs": [],
      "source": [
        "torch.matmul(x, y)  # always uses the last two dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7fxtm64_yq3"
      },
      "outputs": [],
      "source": [
        "x @ y   # still using the last two dimensions since @ overloads matmul"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iod-0Z04_yqx"
      },
      "source": [
        "##### **EXERCISE**\n",
        ">\n",
        "> Use the einsum notation to compute the equivalent broadcast matrix multiplication!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsgxdJnP_yqv"
      },
      "source": [
        "### **Einsum notation**\n",
        "\n",
        "Einstein notation is a way to express complex operations on tensors.\n",
        "\n",
        "- It is **concise but expressive enough** to perform almost every operation you will need in building your neural networks, allowing you to think of the only thing that matters... **dimensions!**\n",
        "- You will **not need to check your dimensions** after an einsum operation, since the dimensions themselves are *defining* the tensor operation.\n",
        "- You will **not need to shape-comment** your tensors. Those comments do not work: they are bound to get outdated.\n",
        "-  You will not need to explicitly code **intermediate operations** such as reshaping, transposing and intermediate tensors.\n",
        "- It is **not library-specific**, being avaiable in ``numpy``, ``pytorch``, ``tensorflow`` and ``jax`` with the same signature. So you do not need to remember the functions signature in all the frameworks.\n",
        "- It can sometimes be compiled to high-performing code (e.g. [Tensor Comprehensions](https://pytorch.org/blog/tensor-comprehensions/))\n",
        "\n",
        "Check [this blog post by Olexa Bilaniuk](https://obilaniu6266h16.wordpress.com/2016/02/04/einstein-summation-in-numpy/) to take a peek under the hood of einsum and [this one by Tim Rocktäschel](https://rockt.github.io/2018/04/30/einsum) for several examples.\n",
        "\n",
        "Its formal behavior is well described in the [Numpy documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html).\n",
        "However, it is very intuitive and better explained through examples.\n",
        "\n",
        "![alt text](https://obilaniu6266h16.files.wordpress.com/2016/02/einsum-fmtstring.png?w=676)\n",
        "\n",
        "> *Historical note (taken from O.Bilaniuk's post)*\n",
        ">\n",
        "> Einstein had no part in the development of this notation. He merely popularized it, by expressing his entire theory of General Relativity in it. In a letter to [Tullio Levi-Civita](https://en.wikipedia.org/wiki/Tullio_Levi-Civita), co-developer alongside [Gregorio Ricci-Curbastro](https://en.wikipedia.org/wiki/Gregorio_Ricci-Curbastro) of Ricci calculus (of which this summation notation was only a part), Einstein wrote:\n",
        ">\n",
        "> \" *I admire the elegance of your method of computation; it must be nice to ride through these fields upon the horse of true mathematics while the like of us have to make our way laboriously on foot.* \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdyM0vLB_yqq"
      },
      "outputs": [],
      "source": [
        "a = torch.arange(6).reshape(2, 3)  # will use this in the examples below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ow4puwv4_yqp"
      },
      "source": [
        "###### **Matrix transpose**\n",
        "\n",
        "$$ B_{ji} = A_{ij} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmE5nSrt_yqk"
      },
      "outputs": [],
      "source": [
        "# The characters are indices along each dimension\n",
        "b = torch.einsum('ij -> ji', a)\n",
        "print_arr(a, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z8j-iVH_yqi"
      },
      "source": [
        "###### **Sum**\n",
        "\n",
        "$$ b = \\sum_i \\sum_j A_{ij} := A_{ij} $$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFjp7cOb_yqd"
      },
      "outputs": [],
      "source": [
        "# Indices that do not appear in the output tensor are summed up\n",
        "b = torch.einsum('ij -> ', a)\n",
        "print_arr(a, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hks-z_NN_yqb"
      },
      "source": [
        "###### **Column sum**\n",
        "\n",
        "$$ b_j = \\sum_i A_{ij} := A_{ij} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXbTLNtL_yqX"
      },
      "outputs": [],
      "source": [
        "# Indices that do not appear in the output tensor are summed up,\n",
        "# ...even if some other index appears\n",
        "b = torch.einsum('ij -> j', a)\n",
        "print_arr(a, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDSB2iy0Mjf3"
      },
      "source": [
        "###### **EXERCISE**\n",
        ">\n",
        "> Given a binary tensor $X \\in \\{0, 1\\}^{n \\times m}$ return a tensor $y \\in \\mathbb{R}^{n}$ that has in the $i$-th position the **number of ones** in the $i$-th row of $X$.\n",
        ">\n",
        ">Give a solution using `einsum`, and a solution using standard manipulation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mj-Vw04DMAyn"
      },
      "outputs": [],
      "source": [
        "x = (torch.rand(100, 200) > 0.5).int()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sewLwDHNdQq"
      },
      "outputs": [],
      "source": [
        "# Display a binary matrix with plotly\n",
        "\n",
        "fig = px.imshow(x)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-YO6eOmrUkK"
      },
      "outputs": [],
      "source": [
        "# ✏️ your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iVQEYEThQn-I"
      },
      "outputs": [],
      "source": [
        "# @title 👀 Solution\n",
        "\n",
        "\n",
        "# Count the number of ones in each row\n",
        "row_ones = torch.einsum('ij -> i', x)\n",
        "\n",
        "row_ones2 = torch.sum(x, dim=-1)  # recall that -1 refers to the last dimension\n",
        "\n",
        "torch.equal(row_ones, row_ones2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUlzmmfTQ_p8"
      },
      "outputs": [],
      "source": [
        "px.imshow(row_ones[:, None]).show()\n",
        "print(f'Sum up the row counts: {row_ones.sum()}\\nSum directly all the ones in the matrix: {x.sum()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA5cWXv3_yqN"
      },
      "source": [
        "###### **Matrix-vector multiplication**\n",
        "\n",
        "$$ c_i = \\sum_k A_{ik}b_k := A_{ik}b_k $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYF4uNUC_yqJ"
      },
      "outputs": [],
      "source": [
        "# Repeated indices in different input tensors indicate pointwise multiplication\n",
        "a = torch.arange(6).reshape(2, 3)\n",
        "b = torch.arange(3)\n",
        "c = torch.einsum('ik, k -> i', [a, b])  # Multiply on k, then sum up on k\n",
        "print_arr(a, b, c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDsla37O_yqH"
      },
      "source": [
        "###### **Matrix-matrix multiplication**\n",
        "\n",
        "$$ C_{ij} = \\sum_k A_{ik}B_{kj} := A_{ik}B_{kj} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8khafOAKElM"
      },
      "source": [
        "📖 Understanding einsum, what happens inside?\n",
        "\n",
        "![alt text](https://obilaniu6266h16.files.wordpress.com/2016/02/einsum-matrixmul.png?w=676)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mcYlmu5_yqD"
      },
      "outputs": [],
      "source": [
        "a = torch.arange(6).reshape(2, 3)\n",
        "b = torch.arange(15).reshape(3, 5)\n",
        "c = torch.einsum('ik, kj -> ij', [a, b])\n",
        "print_arr(a, b, c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVAJP6Ma_yqC"
      },
      "source": [
        "###### **Dot product multiplication**\n",
        "\n",
        "$$ c = \\sum_i a_i b_i := a_i b_i $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2x-XwGOy_yp6"
      },
      "outputs": [],
      "source": [
        "a = torch.arange(3)\n",
        "b = torch.arange(3,6)\n",
        "c = torch.einsum('i,i->', (a, b))\n",
        "print_arr(a, b, c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhLZ4Gl__yp4"
      },
      "source": [
        "###### **Point-wise multiplication**\n",
        "Also known as Hadamard product:\n",
        "\n",
        "$$ C_{ij} = A_{ij} B_{ij} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTUH61Ft_yp0"
      },
      "outputs": [],
      "source": [
        "a = torch.arange(6).reshape(2, 3)\n",
        "b = torch.arange(6,12).reshape(2, 3)\n",
        "c = torch.einsum('ij, ij -> ij', (a, b))\n",
        "print_arr(a, b, c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZoiSCsn_ypz"
      },
      "source": [
        "###### **Outer product**\n",
        "Given two column vectors of length $m$ and $n$ respectively,\n",
        "\\begin{align*}\n",
        "\\mathbf{a}=\\left[\\begin{array}{c}\n",
        "a_{1} &\n",
        "a_{2} &\n",
        "\\dots &\n",
        "a_{m}\n",
        "\\end{array}\\right]^\\top, \\quad \\mathbf{b}=\\left[\\begin{array}{c}\n",
        "b_{1} &\n",
        "b_{2} &\n",
        "\\dots &\n",
        "b_{n}\n",
        "\\end{array}\\right]^\\top\n",
        "\\end{align*}\n",
        "their outer product, denoted $\\mathbf{a} \\otimes \\mathbf{b}$, is defined as the $m \\times n$ matrix $\\mathbf{C}$ obtained by multiplying each element of $\\mathbf{a}$ by each element of $\\mathbf{b}$:\n",
        "\\begin{align*}\n",
        "\\mathbf{a} \\otimes \\mathbf{b}=\\mathbf{C}=\\left[\\begin{array}{cccc}\n",
        "a_{1} b_{1} & a_{1} b_{2} & \\ldots & a_{1} b_{n} \\\\\n",
        "a_{2} b_{1} & a_{2} b_{2} & \\ldots & a_{2} b_{n} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "a_{m} b_{1} & a_{m} b_{2} & \\ldots & a_{m} b_{n}\n",
        "\\end{array}\\right]\n",
        "\\end{align*}\n",
        "Or, in index notation,\n",
        "$$ C_{ij} = a_i b_j $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k71BkJbf_ypu"
      },
      "outputs": [],
      "source": [
        "a = torch.arange(3)\n",
        "b = torch.arange(3,7)\n",
        "c = torch.einsum('i, j -> ij', (a, b))\n",
        "print_arr(a, b, c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MQYFeM2_ypo"
      },
      "outputs": [],
      "source": [
        "# Using the standard PyTorch API\n",
        "torch.outer(a, b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuElOQ9YkjiW"
      },
      "outputs": [],
      "source": [
        "# Using broadcasting black magic\n",
        "a[:, None] * b[None, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXQHwUet_yph"
      },
      "source": [
        "###### 📖 **Batch matrix multiplication**\n",
        "\n",
        "$$ c_{bij} = \\sum_k a_{bik} b_{bkj} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vpbY37H_ypb"
      },
      "outputs": [],
      "source": [
        "a = torch.randn(2,2,5)\n",
        "b = torch.randn(2,5,3)\n",
        "c = torch.einsum('bik,bkj->bij', [a, b])\n",
        "print_arr(a, b, c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fm8KVyoyoNh"
      },
      "source": [
        "###### **EXERCISE**\n",
        "> Implement:\n",
        "> - Matrix transpose with einsum, in particular assume you have a batch of images of shape $(B, C, H, W)$ and you want to turn it into having shape $(B, H, W, C)$\n",
        "> - Quadratic form with einsum  ($y = v^TMv$)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKMXdwUdRN4U"
      },
      "source": [
        "#### 📖 Singleton dimensions\n",
        "\n",
        " In deep learning it is very common to **add or remove dimensions of size $1$** in a tensor. As we mentioned, this is called **unsqueezing** and **squeezing**, and it occurs often during batch processing, manipulating feature maps, making network layers compatible, and in several other occasions.\n",
        "\n",
        " It is possible to perform these operations in different ways, feel free to use\n",
        " whatever is more comfortable to you! Again, **prefer readability to cryptic one-liners** for the sanity of a hypothetical unknown reader or your future self.\n",
        "\n",
        "In the example below, we transform a rank-1 tensor into a rank-2 \"column\", and back to a rank-1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DsWB2_iRwTg"
      },
      "outputs": [],
      "source": [
        "# Define a rank-1 tensor we will use\n",
        "x = torch.arange(6)\n",
        "print_arr(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFqraseqSAhO"
      },
      "source": [
        "Transform **`x` into a column tensor** in four different ways.\n",
        "\n",
        "Remember that the shape of a column tensor is in the form: `(rows, 1)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-BC2G0TSSfO"
      },
      "outputs": [],
      "source": [
        "# 1)\n",
        "# Use the `reshape` or `view` functions\n",
        "\n",
        "y1 = x.reshape(-1, 1)\n",
        "y2 = x.view(-1, 1)\n",
        "\n",
        "print_arr(y1, y2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMOA51cBPp1i"
      },
      "outputs": [],
      "source": [
        "# 2)\n",
        "# Use the specific `unsqueeze` function to unsqueeze a dimension\n",
        "\n",
        "y3 = x.unsqueeze(dim=-1)\n",
        "y4 = x.unsqueeze(dim=1)\n",
        "\n",
        "print_arr(y3, y4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXkhm2nVN8KZ"
      },
      "outputs": [],
      "source": [
        "# 3)\n",
        "# Explicitly index a non-existing dimension with `None`\n",
        "\n",
        "y5 = x[:, None]\n",
        "\n",
        "print_arr(y5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RF7p0CutPQ6"
      },
      "outputs": [],
      "source": [
        "# 4)\n",
        "# Same as before, but do not assume a rank-2 tensor and index the last one.\n",
        "# This approach is useful to write functions that work both for\n",
        "# batched or non-batched data\n",
        "\n",
        "y6 = x[..., None]\n",
        "\n",
        "print_arr(y5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XBTG4CNTcLf"
      },
      "outputs": [],
      "source": [
        "# Now we go back to a rank-1 tensor\n",
        "\n",
        "x1 = y1.reshape(-1)\n",
        "x2 = y2.view(-1)          # Explicity enforce to get a view of the tensors, without copying data\n",
        "x3 = y3.squeeze(dim=-1)\n",
        "x4 = y4.squeeze(dim=1)\n",
        "x5 = y5[:, 0]             # Manually collapse the dimension with an integer indexing\n",
        "x6 = y6[..., 0]\n",
        "\n",
        "print_arr(x1, x2, x3, x4, x5, x6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMVrjaXWySWR"
      },
      "source": [
        "> **NOTE**\n",
        ">\n",
        "> indexing with `...` means  **keeping all the other dimensions the same**.\n",
        "> Keep in mind that `...` is just a Python singleton object (just as `None`).\n",
        "> Its type is Ellipsis:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNKIfvMTsipK"
      },
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BErfK7zVFhAV"
      },
      "outputs": [],
      "source": [
        "x = torch.rand(3,3,3)\n",
        "x[:, :, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eo3ymGK7FhZS"
      },
      "outputs": [],
      "source": [
        "x[..., 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKyFGa7B6jXb"
      },
      "source": [
        "### Tensor types\n",
        "Pay attention to the tensor types!\n",
        "Several methods are available to convert tensors to different types:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHIa9x_X6tnE"
      },
      "outputs": [],
      "source": [
        "a = torch.rand(3, 3) + 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5swdKvAa6w81"
      },
      "outputs": [],
      "source": [
        "a.int()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQfh53W26x6R"
      },
      "outputs": [],
      "source": [
        "a.long()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22VBQVrG64Qq"
      },
      "outputs": [],
      "source": [
        "a.float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSdMzMOb6yk0"
      },
      "outputs": [],
      "source": [
        "a.double()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXKThOrW6zQF"
      },
      "outputs": [],
      "source": [
        "a.bool()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLuyxzT_60Bw"
      },
      "outputs": [],
      "source": [
        "a.to(torch.double)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJDsriTC64-e"
      },
      "outputs": [],
      "source": [
        "a.to(torch.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YRWAuTW7Ybn"
      },
      "outputs": [],
      "source": [
        "a.bool().int()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2InvGnAtJ3nJ"
      },
      "source": [
        "**Pro tip:** Do not try to memorize all the PyTorch API!\n",
        "\n",
        "> Learn to understand what operation should already exist and search for it, when you need it. If it is something common, and it usually is, chances are it already exists.\n",
        "\n",
        "Google, StackOverflow and the docs are your friends!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x0_oylwuIgD"
      },
      "source": [
        "### 📖 Einops\n",
        "\n",
        "If you liked the `einsum` operation, have fun with the [einops](https://github.com/arogozhnikov/einops) package! 🚀\n",
        "\n",
        "It is a third-party library, compatible with most frameworks, that brings superpowers to `einsum`. We will not use the `einops` library in the tutorials, however, feel free to read the [docs](https://github.com/arogozhnikov/einops) and use it.\n",
        "\n",
        "![](http://arogozhnikov.github.io/images/einops/einops_video.gif)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lafVikgz_ypX"
      },
      "source": [
        "### Final exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKKthCbr_dHf"
      },
      "source": [
        "These final exercises are designed to showcase the elegant solutions of einsum.\n",
        "\n",
        "Feel free to also write down solutions that do _not_ use einsum, but rather with standard tensor manipulation!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPeJMqEACs8z"
      },
      "source": [
        "\n",
        "#### **EXERCISE 1**\n",
        ">\n",
        "> You are given $b$ images with size $w \\times h$. Each pixel in each image has three color channels, `(r, g, b)`. These images are organized in a tensor $X \\in \\mathbb{R}^{w \\times b \\times c \\times h}$.\n",
        ">\n",
        "> You want to apply a linear trasformation to the color channel of each single image. In particular, you want to :\n",
        "> - **Convert each image into a grey scale image**.\n",
        "> - **Afterthat, transpose the images** to swap the height and width.\n",
        ">\n",
        "> The linear traformation that converts from `(r, g, b)` to grey scale is simply a linear combination of `r`, `g` and `b`. It can be encoded in the following 1-rank tensor $y \\in \\mathbb{R}^3$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BRybv63oysl"
      },
      "outputs": [],
      "source": [
        "y = torch.tensor([0.2989, 0.5870, 0.1140], dtype=torch.float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ON5HBjD2oyLi"
      },
      "source": [
        "\n",
        "> At the end, you want to obtain a tensor $Z \\in \\mathbb{R}^{b \\times w \\times h}$.\n",
        ">\n",
        "> Write the PyTorch code that performs this operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "If137gBApDcR"
      },
      "outputs": [],
      "source": [
        "# Create the input tensors for the exercise\n",
        "# Execute and ignore this cell\n",
        "\n",
        "from skimage import io\n",
        "from skimage.transform import resize\n",
        "\n",
        "size = 100\n",
        "\n",
        "image1 = io.imread('https://upload.wikimedia.org/wikipedia/commons/thumb/6/6f/Earth_Eastern_Hemisphere.jpg/260px-Earth_Eastern_Hemisphere.jpg')\n",
        "image1 = torch.from_numpy(resize(image1, (size, size), anti_aliasing=True)).float()  # Covert  to float type\n",
        "image1 = image1[..., :3]  # remove alpha channel\n",
        "\n",
        "image2 = io.imread('https://upload.wikimedia.org/wikipedia/commons/thumb/b/b4/The_Sun_by_the_Atmospheric_Imaging_Assembly_of_NASA%27s_Solar_Dynamics_Observatory_-_20100819.jpg/628px-The_Sun_by_the_Atmospheric_Imaging_Assembly_of_NASA%27s_Solar_Dynamics_Observatory_-_20100819.jpg')\n",
        "image2 = torch.from_numpy(resize(image2, (size, size), anti_aliasing=True)).float()\n",
        "image2 = image2[..., :3]  # remove alpha channel\n",
        "\n",
        "image3 = io.imread('https://upload.wikimedia.org/wikipedia/commons/thumb/8/80/Wikipedia-logo-v2.svg/1920px-Wikipedia-logo-v2.svg.png')\n",
        "image3 = torch.from_numpy(resize(image3, (size, size), anti_aliasing=True)).float()\n",
        "image3 = image3[..., :3]  # remove alpha channel\n",
        "\n",
        "source_images = torch.stack((image1, image2, image3), dim=0)\n",
        "images = torch.einsum('bwhc -> wbch', source_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGgBC-dkqwVo"
      },
      "outputs": [],
      "source": [
        "# Plot source images\n",
        "plot_row_images(source_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Flm4DsjMww90"
      },
      "outputs": [],
      "source": [
        "# ✏️ your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NwGXAyrVryTz"
      },
      "outputs": [],
      "source": [
        "# @title 👀 Solution\n",
        "\n",
        "\n",
        "# Grey-fy all images together, using the `images` tensor\n",
        "gray_images = torch.einsum('wbch, c -> bwh', (images, y))\n",
        "\n",
        "# What if you want to transpose the images?\n",
        "gray_images_tr = torch.einsum('wbch, c -> bhw', (images, y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEWp-NDzsB4V"
      },
      "outputs": [],
      "source": [
        "# Plot the gray images\n",
        "plot_row_images(gray_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ga_3wk7BPmp6"
      },
      "outputs": [],
      "source": [
        "# Plot the gray transposed images\n",
        "plot_row_images(gray_images_tr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF6FdNroYMqn"
      },
      "source": [
        "#### **EXERCISE 2**\n",
        ">\n",
        "> Given $k$ points organized in a tensor $X \\in \\mathbb{R}^{k \\times 2}$ apply a reflection along the $y$ axis as a linear transformation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_d9KnJ5Y3e0"
      },
      "outputs": [],
      "source": [
        "# Define some points in R^2\n",
        "x = torch.arange(100, dtype=torch.float)\n",
        "y = x ** 2\n",
        "\n",
        "# Define some points in R^2\n",
        "data = torch.stack((x, y), dim=0).t()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0_JB4C1Y2vm"
      },
      "outputs": [],
      "source": [
        "px.scatter(x = data[:, 0].numpy(), y = data[:, 1].numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1Xt5XEkDYvB"
      },
      "outputs": [],
      "source": [
        "# ✏️ your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "eWCEwmIfaebY"
      },
      "outputs": [],
      "source": [
        "# @title 👀 Solution\n",
        "\n",
        "\n",
        "# Define a matrix that encodes a linear transformation\n",
        "S = torch.tensor([[-1, 0],\n",
        "                  [ 0, 1]], dtype=torch.float)\n",
        "\n",
        "# Apply the linear transformation: the order is important\n",
        "new_data = torch.einsum('nk, kd -> nd', (data, S))\n",
        "\n",
        "# Double check yourself:\n",
        "# The linear transformation correctly maps the basis vectors!\n",
        "S @ torch.tensor([[0],\n",
        "                  [1]], dtype=torch.float)\n",
        "S @ torch.tensor([[1],\n",
        "                  [0]], dtype=torch.float)\n",
        "\n",
        "# Check if at least the shape is correct\n",
        "new_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5H7VEJFXbaMb"
      },
      "outputs": [],
      "source": [
        "# Plot the new points\n",
        "px.scatter(x = new_data[:, 0].numpy(), y = new_data[:, 1].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1tHmuIBM9ka"
      },
      "source": [
        "#### **EXERCISE 3**\n",
        ">\n",
        ">  You are given $b$ images with size $w \\times h$. Each pixel in each image has `(r, g, b)` channels. These images are organized in a tensor $X \\in \\mathbb{R}^{w \\times b \\times c \\times h}$, i.e. the same tensor as in the exercise 1.\n",
        ">\n",
        "> You want to swap the `red` color with the `blue` color, and decrease the intensity of the `green` by half.\n",
        ">\n",
        "> Perform the transformation on all the images simultaneously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lppGxeMSO0c8"
      },
      "outputs": [],
      "source": [
        "images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POWStC54w2ZZ"
      },
      "outputs": [],
      "source": [
        "# ✏️ your code her"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VE2xc7i_ORzg"
      },
      "outputs": [],
      "source": [
        "# @title 👀 Solution\n",
        "\n",
        "\n",
        "# Define the linear transformation to swap the blue and red colors\n",
        "# and half the green\n",
        "S = torch.tensor([[ 0, 0, 1],\n",
        "                  [ 0, .5, 0],\n",
        "                  [ 1, 0, 0]], dtype=torch.float)\n",
        "\n",
        "# Apply the linear transformation to the color channel!\n",
        "rb_images = torch.einsum('wbch, dc -> wbdh', (images, S))\n",
        "\n",
        "# Let's try to read into the einsum operation above.\n",
        "#\n",
        "# For each pixel p=(r,g,b) in each image of the images tensor, we want to apply\n",
        "# the transformation S @ p. In einsum notation, this would be a transformation\n",
        "# represented as 'dc, c -> d' or equivalently 'c, dc -> d' if we swap the order\n",
        "# of the two arguments.\n",
        "#\n",
        "# Now, our pixels are stored in a 4d tensor with dimensions wbch. All we want\n",
        "# einsum to do, is loop over the dimensions w, b, and h, extract a pixel at each\n",
        "# iteration, and apply the 'c, dc -> d' transformation to it.\n",
        "#\n",
        "# So here's how all the above would read in einsum notation:\n",
        "#   'wbch, dc -> wbdh'\n",
        "# Basically the w, b, h dimensions stay the same (we're just looping over them),\n",
        "# while the c dimension gets transformed to d by application of S.\n",
        "\n",
        "# Finally permute the dimensions so that we can use plot_row_images().\n",
        "rb_images = rb_images.permute(1, 0, 3, 2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 👀 Alternative solution\n",
        "\n",
        "\n",
        "S = torch.tensor([[ 0, 0, 1],\n",
        "                  [ 0, .5, 0],\n",
        "                  [ 1, 0, 0]], dtype=torch.float)\n",
        "\n",
        "# Instead of explicitly permuting the dimensions of the final result\n",
        "# to make it work with plot_row_images(), we can directly ask einsum\n",
        "# to apply a permutation for us!\n",
        "#\n",
        "# We just change the output dimensions 'wbdh' to 'bwhd':\n",
        "rb_images = torch.einsum('wbch, dc -> bwhd', (images, S))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nEmC4GwlwxhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zdfYOMBOxjv"
      },
      "outputs": [],
      "source": [
        "plot_row_images(rb_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLCQReiBPLqd"
      },
      "source": [
        "#### 📖 **EXERCISE 4**\n",
        ">\n",
        ">  You are given $b$ images with size $w \\times h$. Each pixel in each image has `(r, g, b)` colors. These images are organized in a tensor $X \\in \\mathbb{R}^{w \\times b \\times c \\times h}$, i.e. the same tensor as exercise 1 and 3.\n",
        ">\n",
        "> You want to **convert each image into a 3D point cloud**:\n",
        "> - the `(x, y)` coordinates of each point in the point cloud are the **indices** of the pixels in the original image\n",
        "> - the `z` coordinate of each point in the point cloud is the $L_2$ norm of the color of the corresponding pixel, multiplied by $10$\n",
        ">\n",
        "> *Hint*: you may need some other PyTorch function, search the docs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Sk-O_iHgSlbX"
      },
      "outputs": [],
      "source": [
        "# @title 👀 Solution\n",
        "\n",
        "\n",
        "# Just normalize the tensor into the common form [batch, width, height, colors]\n",
        "imgs = torch.einsum('wbch -> bwhc', images)\n",
        "imgs.shape\n",
        "\n",
        "# The x, y coordinate of the point cloud are all the possible pairs of indices (i, j)\n",
        "row_indices = torch.arange(imgs.shape[1], dtype=torch.float)\n",
        "col_indices = torch.arange(imgs.shape[2], dtype=torch.float)\n",
        "xy = torch.cartesian_prod(row_indices , col_indices)\n",
        "\n",
        "# Compute the L2 norm for each pixel in each image\n",
        "depth = imgs.norm(p=2, dim = -1)\n",
        "# depth = torch.einsum('bwhc, bwhc -> bwh', (imgs, imgs)) ** (1/2)\n",
        "\n",
        "# For every pair (i, j), retrieve the L2 norm of that pixel\n",
        "z = depth[:, xy[:, 0].long(), xy[:, 1].long()] * 10\n",
        "\n",
        "# Adjust the dimensions, repeat and concatenate accordingly\n",
        "xy = xy.repeat(imgs.shape[0], 1, 1)  # x,y coordinates are constant for the three images\n",
        "clouds = torch.cat((xy, z[..., None] ), dim= 2)\n",
        "\n",
        "# Three images, 10000 points, each point with coordinates x,y,z in 3D\n",
        "clouds.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1S0OYtiZgT_"
      },
      "outputs": [],
      "source": [
        "# Utility function\n",
        "# Execute and ignore this cell\n",
        "\n",
        "from typing import Union\n",
        "\n",
        "def plot_3d_point_cloud(cloud: Union[torch.Tensor, np.ndarray]) -> None:\n",
        "  \"\"\" Plot a single 3D point cloud\n",
        "\n",
        "  :param cloud: tensor with shape [number of points, coordinates]\n",
        "  \"\"\"\n",
        "  import pandas as pd\n",
        "  df = pd.DataFrame(np.asarray(cloud), columns=['x', 'y', 'z'])\n",
        "  fig = px.scatter_3d(df, x=df.x, y=df.y, z=df.z, color=df.z, opacity=1, range_z=[0, 30])\n",
        "  fig.update_layout({'scene_aspectmode': 'data', 'scene_camera':  dict(\n",
        "          up=dict(x=0., y=0., z=0.),\n",
        "          eye=dict(x=0., y=0., z=3.)\n",
        "      )})\n",
        "  fig.update_traces(marker=dict(size=3,),\n",
        "                    selector=dict(mode='markers'))\n",
        "  _ = fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlKzABIKm9UZ"
      },
      "outputs": [],
      "source": [
        "plot_3d_point_cloud(clouds[0, ...])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5Jm-zYSbQYU"
      },
      "outputs": [],
      "source": [
        "plot_3d_point_cloud(clouds[1, ...])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TH0CITRgZnyZ"
      },
      "outputs": [],
      "source": [
        "plot_3d_point_cloud(clouds[2, ...])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ivMykCiurNH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ezcEIBPFSlFa",
        "HKMXdwUdRN4U"
      ],
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}